{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4a55df-4431-4a2c-8dd2-98284a965ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /scratch/mmm9912/hf_cache\n",
      "HF_DATASETS_CACHE: /scratch/mmm9912/hf_cache/datasets\n",
      "XDG_CACHE_HOME: /scratch/mmm9912/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/mmm9912/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/scratch/mmm9912/hf_cache/datasets\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/scratch/mmm9912/hf_cache\"\n",
    "\n",
    "# Verify that the cache paths are set\n",
    "for var in [\"HF_HOME\", \"HF_DATASETS_CACHE\", \"XDG_CACHE_HOME\"]:\n",
    "    print(f\"{var}: {os.environ.get(var)}\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d41d0f4-2da9-4a46-a928-f2f27a65fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mmm9912/condaDONOTDELETE/qwen25vl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2bf8a5-4412-4a97-ae50-e536cf293c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:43<00:00,  8.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    load_in_8bit=True,          # Enable 8-bit quantization for inference\n",
    "    device_map=\"sequential\",    # Load layers sequentially\n",
    "    offload_folder=\"/scratch/mmm9912/qwen25vl_offload\"  # Offload weights to CPU as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5ef95a-458e-4f64-ba3b-b783c63a1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also lower the expected visual resolution\n",
    "min_pixels = 128 * 28 * 28   # Drastically reduced from defaults\n",
    "max_pixels = 256 * 28 * 28\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3141f05-06db-4477-a5fa-f539d41ff6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(model, processor, inputs):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,  # Reduced from 1000\n",
    "            temperature=0.1,      # Added for determinism\n",
    "            top_p=0.9,\n",
    "            do_sample=False,      # Use greedy decoding\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        # Decode the generated tokens into text\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        return output_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1f09149-17fc-4f28-b430-546bb3ead2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(db_messages):\n",
    "    \"\"\"\n",
    "    Given a list of messages from the database, format them into the proper conversation structure.\n",
    "    \n",
    "    Each message is represented as a dict with a role and a list of content blocks.\n",
    "    The conversation MUST begin with a system message that instructs the assistant to output\n",
    "    a single valid JSON object with exactly these three keys:\n",
    "      - \"direct_answer_to_frontend\": a string that will be sent to the user.\n",
    "      - \"function_call_to_video_processor\": an object (with \"name\" and \"arguments\") to trigger frame extraction, or null.\n",
    "      - \"function_call_to_extracted_frames_processor\": an object (with \"name\" and \"arguments\") to trigger expert inference, or null.\n",
    "    \n",
    "    Allowed expert inference options:\n",
    "      - For frequency inconsistencies (SPSL/frequency), use model_name \"spsl\".\n",
    "      - For spatial inconsistencies (UCF/spatial), use model_name \"ucf\".\n",
    "      - For general inconsistencies (naïve/xception), use model_name \"xception\".\n",
    "    \n",
    "    IMPORTANT:\n",
    "      - NEVER use your own reasoning to produce a final answer unless no expert analysis is required.\n",
    "      - Use the video solely for contextual description.\n",
    "      - If the user's request requires expert analysis (e.g. \"What does SPSL say about this video?\"), \n",
    "        you MUST output a JSON function call (with the appropriate inference option) instead of a final answer.\n",
    "      - Your output MUST be ONLY a valid JSON object (with no extra text, markdown formatting, code fences, or commentary).\n",
    "    \n",
    "    EXACT EXAMPLES (output exactly as shown):\n",
    "    \n",
    "    *For frame extraction:*\n",
    "    {\n",
    "      \"direct_answer_to_frontend\": \"\",\n",
    "      \"function_call_to_video_processor\": {\n",
    "         \"name\": \"extract_frames_from_video\",\n",
    "         \"arguments\": {\n",
    "            \"video_path\": \"<path_to_video>\",\n",
    "            \"num_frames\": 4\n",
    "         }\n",
    "      },\n",
    "      \"function_call_to_extracted_frames_processor\": null\n",
    "    }\n",
    "    \n",
    "    *For expert inference (e.g. SPSL/frequency):*\n",
    "    {\n",
    "      \"direct_answer_to_frontend\": \"\",\n",
    "      \"function_call_to_video_processor\": null,\n",
    "      \"function_call_to_extracted_frames_processor\": {\n",
    "         \"name\": \"run_inference_on_images_with_old_preprocess\",\n",
    "         \"arguments\": {\n",
    "            \"model_name\": \"spsl\",\n",
    "            \"image_paths\": [\"<path1>\", \"<path2>\", \"...\"],\n",
    "            \"cuda\": true,\n",
    "            \"manual_seed\": 42\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    *For a final answer (when no expert analysis is required):*\n",
    "    {\n",
    "      \"direct_answer_to_frontend\": \"<your final answer>\",\n",
    "      \"function_call_to_video_processor\": null,\n",
    "      \"function_call_to_extracted_frames_processor\": null\n",
    "    }\n",
    "    \n",
    "    Follow these instructions exactly.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                \"You MUST respond with EXACTLY one JSON object containing: \"\n",
    "                \"'direct_answer_to_frontend', 'function_call_to_video_processor', \"\n",
    "                \"and 'function_call_to_extracted_frames_processor'. \"\n",
    "                \"NEVER include markdown, additional text, or explanations. \"\n",
    "                \"Follow these examples rigidly:\\n\\n\"\n",
    "                \"User asks about models: \"\n",
    "                '{\"direct_answer_to_frontend\": \"\", '\n",
    "                '\"function_call_to_video_processor\": null, '\n",
    "                '\"function_call_to_extracted_frames_processor\": {'\n",
    "                '\"name\": \"run_inference...\", \"arguments\": {\"model_name\": \"spsl\"}}}\\n\\n'\n",
    "                \"User says 'Hello': \"\n",
    "                '{\"direct_answer_to_frontend\": \"Hello! How can I assist?\", '\n",
    "                '\"function_call...\": null}'\n",
    "            )\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    formatted = [system_message]\n",
    "    sorted_messages = sorted(db_messages, key=lambda msg: msg[\"id\"])\n",
    "    \n",
    "    for msg in sorted_messages:\n",
    "        entry = {\"role\": msg.get(\"role\", \"user\"), \"content\": []}\n",
    "        if msg.get(\"media_type\") in [\"image\", \"video\"] and msg.get(\"media_url\"):\n",
    "            entry[\"content\"].append({\n",
    "                \"type\": msg[\"media_type\"],\n",
    "                \"image\" if msg[\"media_type\"] == \"image\" else \"video\": f\"file://{msg['media_url']}\"\n",
    "            })\n",
    "        text = msg.get(\"content\", \"\").strip()\n",
    "        if text:\n",
    "            entry[\"content\"].append({\"type\": \"text\", \"text\": text})\n",
    "        formatted.append(entry)\n",
    "    \n",
    "    return formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "276fcbb2-9c50-4538-937e-cc1812943851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import uuid\n",
    "\n",
    "def extract_frames_from_video(video_path, num_frames=4, output_dir=\"/scratch/mmm9912/Capstone/FRONT_END_STORAGE/images/\"):\n",
    "    \"\"\"\n",
    "    Extracts up to `num_frames` equally spaced frames from the provided video.\n",
    "    If the video has fewer than num_frames, extracts all available frames.\n",
    "    Saves each extracted frame as a PNG with a random unique filename in output_dir.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path (str): Full path to the video file.\n",
    "        num_frames (int): Desired number of frames to extract.\n",
    "        output_dir (str): Directory in which to save the frames.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of file paths for the extracted frame images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Cannot open video: \" + video_path)\n",
    "        \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Determine indices: if fewer frames than desired, use them all.\n",
    "    if total_frames <= num_frames:\n",
    "        indices = list(range(total_frames))\n",
    "    else:\n",
    "        indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
    "    \n",
    "    extracted_paths = []\n",
    "    current_frame = 0\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if current_frame in indices:\n",
    "            filename = uuid.uuid4().hex + \".png\"\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(file_path, frame)\n",
    "            extracted_paths.append(file_path)\n",
    "        current_frame += 1\n",
    "    cap.release()\n",
    "    return extracted_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b221baa3-78eb-41cf-af7d-0de88e245691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_formatter(inference_results):\n",
    "    \"\"\"\n",
    "    Given a list of inference_results (each a tuple of (overlay_path, confidence, prediction_message, red_percentage)),\n",
    "    produce a list of message dictionaries. For each result, a message containing text and the overlay image is produced,\n",
    "    followed by a final message that summarizes the overall analysis.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of message dictionaries.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    # For each inference result (assumed to be one per extracted frame)\n",
    "    for overlay_path, confidence, prediction_message, red_percentage in inference_results:\n",
    "        text_message = (\n",
    "            f\"Frame analysis: {prediction_message} \"\n",
    "            f\"(Confidence: {confidence:.2f}, Red activation: {red_percentage:.2f}%).\"\n",
    "        )\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": text_message},\n",
    "                {\"type\": \"image\", \"image\": f\"file://{overlay_path}\"}\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Final conclusive message\n",
    "    final_conclusion = (\n",
    "        \"Based on the analyses of the extracted frames, the expert model indicates a high likelihood \"\n",
    "        \"of deepfake manipulation. Please consider further forensic evaluation.\"\n",
    "    )\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": final_conclusion}\n",
    "        ]\n",
    "    })\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22858951-924a-43d7-b7c0-c45050f27d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_images_with_old_preprocess(model_name, image_paths, cuda, manual_seed):\n",
    "    \"\"\"\n",
    "    Simulated function call to run inference on images with old preprocess.\n",
    "    In production, this function would run the deepfake detection model.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): One of \"spsl\", \"ucf\", or \"xception\".\n",
    "        image_paths (list): List of image paths.\n",
    "        cuda (bool): Whether to use CUDA.\n",
    "        manual_seed (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of 4 tuples, each tuple containing:\n",
    "            - overlay_path (str): Path to the Grad-CAM overlay image.\n",
    "            - confidence (float): Softmax probability that the image is forged.\n",
    "            - prediction_message (str): Verdict message from the model.\n",
    "            - red_percentage (float): Percentage of red pixels in the Grad-CAM heatmap.\n",
    "    \"\"\"\n",
    "    overlay_path = \"/scratch/mmm9912/Capstone/FRONT_END_STORAGE/images/ca4227e5f59643179b25ba59c0483b9b.png\"\n",
    "    confidence = 0.75\n",
    "    prediction_message = f\"{model_name.upper()} model detected forgery.\"\n",
    "    red_percentage = 10.0\n",
    "    # Duplicate the dummy result 4 times.\n",
    "    return [(overlay_path, confidence, prediction_message, red_percentage) for _ in range(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42903722-4f3c-4dd0-a54c-f43a056ad09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(response_text):\n",
    "    # Attempt strict extraction of a JSON object from the response text.\n",
    "    json_match = re.search(r\"(\\{.*\\})\", response_text, re.DOTALL)\n",
    "    if not json_match:\n",
    "        raise ValueError(\"No JSON found in response\")\n",
    "    \n",
    "    json_str = json_match.group(1)\n",
    "    try:\n",
    "        parsed = json.loads(json_str)\n",
    "        # Validate that all required keys are present.\n",
    "        assert all(k in parsed for k in [\n",
    "            \"direct_answer_to_frontend\",\n",
    "            \"function_call_to_video_processor\",\n",
    "            \"function_call_to_extracted_frames_processor\"\n",
    "        ])\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON structure: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# When an invalid JSON is detected, use a retry prompt:\n",
    "def retry_with_correction(model, processor, conversation_messages):\n",
    "    retry_prompt = (\n",
    "        \"Your previous response was invalid. You MUST respond with ONLY JSON matching \"\n",
    "        \"the required format. Example valid response:\\n\"\n",
    "        '{\"direct_answer_to_frontend\": \"\", '\n",
    "        '\"function_call_to_video_processor\": null, '\n",
    "        '\"function_call_to_extracted_frames_processor\": null}'\n",
    "    )\n",
    "    correction_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": retry_prompt\n",
    "        }]\n",
    "    }\n",
    "    # Append the correction prompt to the conversation history.\n",
    "    updated_messages = conversation_messages + [correction_message]\n",
    "    text = processor.apply_chat_template(updated_messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "    return answer(model, processor, inputs)\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 4. Enhanced Keyword Detection            #\n",
    "###########################################\n",
    "def enforce_inference_call(last_user_text, response_json):\n",
    "    # Define expert keyword groups mapping to inference models.\n",
    "    expert_keywords = {\n",
    "        \"spsl\": [\"spsl\", \"frequency\", \"spectral\"],\n",
    "        \"ucf\": [\"ucf\", \"spatial\", \"artifact\"],\n",
    "        \"xception\": [\"xception\", \"naïve\", \"general\"]\n",
    "    }\n",
    "    frames_processor_call = response_json.get(\"function_call_to_extracted_frames_processor\")\n",
    "    # If expert keywords are present but no inference function call exists, force one.\n",
    "    for model_name, keywords in expert_keywords.items():\n",
    "        if any(kw in last_user_text.lower() for kw in keywords) and frames_processor_call is None:\n",
    "            return {\n",
    "                \"direct_answer_to_frontend\": \"\",\n",
    "                \"function_call_to_video_processor\": None,\n",
    "                \"function_call_to_extracted_frames_processor\": {\n",
    "                    \"name\": \"run_inference_on_images_with_old_preprocess\",\n",
    "                    \"arguments\": {\"model_name\": model_name}\n",
    "                }\n",
    "            }\n",
    "    return response_json\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 5. Conversation History Cleaning         #\n",
    "###########################################\n",
    "def clean_conversation_history(messages):\n",
    "    \"\"\"Remove error messages and non-essential system messages.\"\"\"\n",
    "    return [msg for msg in messages if not (\n",
    "        msg.get(\"role\") == \"system\" and \n",
    "        \"SAMPLE RESPONSE FORMAT:\" in msg.get(\"content\", [{}])[0].get(\"text\", \"\")\n",
    "    )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec6a2a00-a5d5-4386-87d6-c5e153046f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pprint\n",
    "import json\n",
    "from supabase_wrapper import get_all_messages, create_conversation, insert_message\n",
    "\n",
    "node_dir = \"/scratch/mmm9912/condaDONOTDELETE/qwen25vl/bin\"\n",
    "os.environ[\"PATH\"] = node_dir + os.pathsep + os.environ[\"PATH\"]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# If __file__ is not defined (e.g. in a notebook), use the current working directory.\n",
    "try:\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6d8d619a-660d-424f-848a-dd5d1ceba71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial highest message id: 250\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Get the initial list of messages and determine the current highest message id.\n",
    "initial_data = get_all_messages()\n",
    "messages = initial_data.get(\"messages\", [])\n",
    "\n",
    "highest_id = max((message[\"id\"] for message in messages), default=0)  # Ensure it starts from 0 if no messages exist\n",
    "\n",
    "print(\"Initial highest message id:\", highest_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bbdd3-253b-488f-aae3-56a330c0e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    data = get_all_messages()\n",
    "    messages = data.get(\"messages\", [])\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"No messages found in the database. Waiting...\")\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    current_highest = max(message[\"id\"] for message in messages)\n",
    "    if current_highest == highest_id:\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    new_message = next((msg for msg in messages if msg[\"id\"] == current_highest), None)\n",
    "    if new_message is None:\n",
    "        print(\"Error: Highest id message not found. Waiting...\")\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    # Only trigger a cycle if the new message is from a user.\n",
    "    if new_message.get(\"role\", \"\").lower() != \"user\":\n",
    "        print(\"New message is not from a user. Skipping cycle.\")\n",
    "        highest_id = current_highest\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    conversation_id = new_message[\"conversation_id\"]\n",
    "    conversation_messages = [msg for msg in messages if msg[\"conversation_id\"] == conversation_id]\n",
    "    conversation_messages_sorted = clean_conversation_history(\n",
    "        sorted(conversation_messages, key=lambda msg: msg[\"id\"])\n",
    "    )\n",
    "    print(f\"New message detected in conversation {conversation_id}.\")\n",
    "\n",
    "    # Format the conversation (this prepends the strict system prompt).\n",
    "    formatted_messages = format_conversation(conversation_messages_sorted)\n",
    "    \n",
    "    # Inject a sample response format message immediately after the user's last input.\n",
    "    if conversation_messages_sorted[-1].get(\"role\", \"\").lower() == \"user\":\n",
    "        sample_response_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"SAMPLE RESPONSE FORMAT:\\n\"\n",
    "                        \"If you receive a video, and ONLY if you receive a video, first perform frame extraction:\\n\"\n",
    "                        '{\"direct_answer_to_frontend\": \"\", \"function_call_to_video_processor\": {\"name\": \"extract_frames_from_video\", \"arguments\": {\"video_path\": \"<path_to_video>\", \"num_frames\": 4}}, \"function_call_to_extracted_frames_processor\": null}\\n'\n",
    "                        \"Then send for expert inference (e.g. SPSL/frequency):\\n\"\n",
    "                        '{\"direct_answer_to_frontend\": \"\", \"function_call_to_video_processor\": null, \"function_call_to_extracted_frames_processor\": {\"name\": \"run_inference_on_images_with_old_preprocess\", \"arguments\": {\"model_name\": \"spsl\", \"image_paths\": [\"<path1>\", \"<path2>\", \"...\"], \"cuda\": true, \"manual_seed\": 42}}}\\n'\n",
    "                        \"For a final answer or remaining conversational. ALWAYS remain conversational when your response is directed at the user:\\n\"\n",
    "                        '{\"direct_answer_to_frontend\": \"<your final answer>\", \"function_call_to_video_processor\": null, \"function_call_to_extracted_frames_processor\": null}\\n'\n",
    "                        \"Follow these instructions exactly, while always writing grammatically correct sentences when your response is directed to the user. The user can never see the internal workings of your processes. Now tackle the last user query within the JSON format.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        formatted_messages.append(sample_response_message)\n",
    "        \n",
    "    print(\"Formatted messages:\", formatted_messages)\n",
    "    \n",
    "    # Build the conversation prompt.\n",
    "    text = processor.apply_chat_template(formatted_messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(formatted_messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs if image_inputs else None,\n",
    "        videos=video_inputs if video_inputs else None,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    raw_response_text = answer(model, processor, inputs)\n",
    "    print(\"Raw VLM response:\")\n",
    "    print(raw_response_text)\n",
    "    \n",
    "    # Attempt to parse the JSON response, with a retry on error.\n",
    "    try:\n",
    "        response_json = parse_json_response(raw_response_text)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from VLM response:\", e)\n",
    "        # Retry using the correction prompt.\n",
    "        raw_response_text = retry_with_correction(model, processor, formatted_messages)\n",
    "        try:\n",
    "            response_json = parse_json_response(raw_response_text)\n",
    "        except Exception as inner_e:\n",
    "            print(\"Retry failed:\", inner_e)\n",
    "            insert_message(\n",
    "                conversation_id=conversation_id,\n",
    "                role=\"assistant\",\n",
    "                content=\"An error occurred while processing the response.\",\n",
    "                media_url=None,\n",
    "                media_type=None\n",
    "            )\n",
    "            highest_id = current_highest\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    \n",
    "    direct_answer = response_json.get(\"direct_answer_to_frontend\", \"\")\n",
    "    video_processor_call = response_json.get(\"function_call_to_video_processor\")\n",
    "    frames_processor_call = response_json.get(\"function_call_to_extracted_frames_processor\")\n",
    "    \n",
    "    # Safely extract the latest user text for keyword checking.\n",
    "    last_user_text = \"\"\n",
    "    for msg in reversed(conversation_messages_sorted):\n",
    "        if msg.get(\"role\", \"\").lower() == \"user\":\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if isinstance(content, list):\n",
    "                for block in content:\n",
    "                    if isinstance(block, dict) and block.get(\"type\") == \"text\":\n",
    "                        last_user_text += block.get(\"text\", \"\")\n",
    "                    elif isinstance(block, str):\n",
    "                        last_user_text += block\n",
    "            elif isinstance(content, str):\n",
    "                last_user_text += content\n",
    "            if last_user_text:\n",
    "                break\n",
    "\n",
    "    # If expert keywords are detected but no inference call is provided,\n",
    "    # force the appropriate inference function call.\n",
    "    expert_keywords = [\"spsl\", \"frequency\", \"ucf\", \"spatial\", \"xception\", \"naïve\"]\n",
    "    if any(kw in last_user_text.lower() for kw in expert_keywords) and frames_processor_call is None:\n",
    "        print(\"Expert analysis query detected without inference call; forcing inference call.\")\n",
    "        response_json = enforce_inference_call(last_user_text, response_json)\n",
    "        frames_processor_call = response_json.get(\"function_call_to_extracted_frames_processor\")\n",
    "    \n",
    "    # Chain 1: Handle video processing call (frame extraction).\n",
    "    if video_processor_call is not None:\n",
    "        func_name = video_processor_call.get(\"name\")\n",
    "        func_args = video_processor_call.get(\"arguments\", {})\n",
    "        if func_name == \"extract_frames_from_video\":\n",
    "            try:\n",
    "                extracted_frames = extract_frames_from_video(**func_args)\n",
    "                function_result_message = {\n",
    "                    \"role\": \"function\",\n",
    "                    \"name\": func_name,\n",
    "                    \"content\": json.dumps({\"extracted_frames\": extracted_frames})\n",
    "                }\n",
    "                formatted_messages.append(function_result_message)\n",
    "                text = processor.apply_chat_template(formatted_messages, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs if image_inputs else None,\n",
    "                    videos=video_inputs if video_inputs else None,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(model.device)\n",
    "                raw_response_text = answer(model, processor, inputs)\n",
    "                print(\"VLM response after frame extraction:\")\n",
    "                print(raw_response_text)\n",
    "                try:\n",
    "                    response_json = parse_json_response(raw_response_text)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(\"Error after frame extraction: \" + str(e))\n",
    "                direct_answer = response_json.get(\"direct_answer_to_frontend\", \"\")\n",
    "                frames_processor_call = response_json.get(\"function_call_to_extracted_frames_processor\")\n",
    "            except Exception as e:\n",
    "                print(\"Error executing extract_frames_from_video:\", e)\n",
    "    \n",
    "    # Chain 2: Handle inference call (expert analysis using extracted frames).\n",
    "    if frames_processor_call is not None:\n",
    "        func_name = frames_processor_call.get(\"name\")\n",
    "        func_args = frames_processor_call.get(\"arguments\", {})\n",
    "        if func_name == \"run_inference_on_images_with_old_preprocess\":\n",
    "            try:\n",
    "                inference_results = run_inference_on_images_with_old_preprocess(**func_args)\n",
    "                formatted_responses = response_formatter(inference_results)\n",
    "                for resp in formatted_responses:\n",
    "                    insert_message(\n",
    "                        conversation_id=conversation_id,\n",
    "                        role=\"assistant\",\n",
    "                        content=json.dumps(resp),\n",
    "                        media_url=None,\n",
    "                        media_type=None\n",
    "                    )\n",
    "                print(\"Inserted formatted responses from inference results.\")\n",
    "                highest_id = current_highest\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(\"Error executing run_inference_on_images_with_old_preprocess:\", e)\n",
    "    \n",
    "    # If no function call is triggered and a direct answer is provided,\n",
    "    # insert ONLY the string inside \"direct_answer_to_frontend\" into the database.\n",
    "    if direct_answer and (video_processor_call is None and frames_processor_call is None):\n",
    "        insert_message(\n",
    "            conversation_id=conversation_id,\n",
    "            role=\"assistant\",\n",
    "            content=direct_answer,\n",
    "            media_url=None,\n",
    "            media_type=None\n",
    "        )\n",
    "        print(\"Inserted direct answer to frontend.\")\n",
    "    \n",
    "    print(\"Cycle complete.\\n\")\n",
    "    highest_id = current_highest\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317d1ba-8f75-41df-ab01-a2fe4c29d902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0b57b-7ad9-41b4-ba9b-699ea59f9152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c1c5a-8c1c-4f4c-83ff-83dcde34470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589e645-f639-43f0-9331-9f3dcc06f6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecd0cd-6054-42c6-8110-383430413c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ab58a-3c4f-4b43-8e77-5b4b7334893e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4a23b-332d-4ba2-a004-34d4f5e11c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen25vl)",
   "language": "python",
   "name": "qwen25vl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
