{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4a55df-4431-4a2c-8dd2-98284a965ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /scratch/mmm9912/hf_cache\n",
      "HF_DATASETS_CACHE: /scratch/mmm9912/hf_cache/datasets\n",
      "XDG_CACHE_HOME: /scratch/mmm9912/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/mmm9912/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/scratch/mmm9912/hf_cache/datasets\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/scratch/mmm9912/hf_cache\"\n",
    "\n",
    "# Verify that the cache paths are set\n",
    "for var in [\"HF_HOME\", \"HF_DATASETS_CACHE\", \"XDG_CACHE_HOME\"]:\n",
    "    print(f\"{var}: {os.environ.get(var)}\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d41d0f4-2da9-4a46-a928-f2f27a65fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mmm9912/condaDONOTDELETE/qwen25vl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb2bf8a5-4412-4a97-ae50-e536cf293c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    load_in_8bit=True,          # Enable 8-bit quantization for inference\n",
    "    device_map=\"sequential\",    # Load layers sequentially\n",
    "    offload_folder=\"/scratch/mmm9912/qwen25vl_offload\"  # Offload weights to CPU as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a5ef95a-458e-4f64-ba3b-b783c63a1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also lower the expected visual resolution\n",
    "min_pixels = 128 * 28 * 28   # Drastically reduced from defaults\n",
    "max_pixels = 256 * 28 * 28\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    min_pixels=min_pixels,\n",
    "    max_pixels=max_pixels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3141f05-06db-4477-a5fa-f539d41ff6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(model, processor, inputs):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000\n",
    "        )\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        # Decode the generated tokens into text\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        return output_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b280b224-633b-4727-92ee-09fc872905f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "   system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"You are a forensic analyst specializing in deepfake detection. \"\n",
    "                    \"Your job is to analyze the provided image and determine if it comes from a deepfake video. \"\n",
    "                    \"Stick to the expert models' outputs and visible context—ignore compression and image quality artifacts. \"\n",
    "                    \"Keep your responses clear, structured, and to the point. \"\n",
    "                    \"If the user asks generally whether the image is real or a deepfake, give a **straightforward yet conversational answer** \"\n",
    "                    \"without listing numbers or technical breakdowns. \"\n",
    "                    \"If they ask for specifics (like spatial, temporal, or frequency details), then include only the requested information. \"\n",
    "                    \"Do not provide numbers or breakdowns unless explicitly asked.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"Expert Model Outputs (for reference, **not to be shared unless explicitly requested**):\\n\"\n",
    "                    \"- Frequency model (SPSL): 27% of pixels flagged, 97% confidence (deepfake).\\n\"\n",
    "                    \"- Temporal model (FlowFormer++): 17% of pixels flagged, 12% confidence (real).\\n\"\n",
    "                    \"- Spatial model (UCF): 1% of pixels flagged, 88% confidence (real).\\n\"\n",
    "                    \"Overall Prediction: Real.\\n\"\n",
    "                    \"Final call is yours—models aren't always right.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1f09149-17fc-4f28-b430-546bb3ead2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(db_messages):\n",
    "    \"\"\"\n",
    "    Given a list of messages from the database, format them into the proper conversation structure.\n",
    "    \n",
    "    Parameters:\n",
    "        db_messages (list): A list of dictionaries, each representing a message. Each dictionary should have:\n",
    "            - \"id\": Unique message id.\n",
    "            - \"conversation_id\": Conversation identifier.\n",
    "            - \"role\": \"user\" or \"assistant\" (or even \"system\" if present).\n",
    "            - \"content\": The text content.\n",
    "            - \"media_url\": (Optional) Full path to an image.\n",
    "            - \"media_type\": (Optional) Type of media (e.g., \"image\").\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries in the following format:\n",
    "        \n",
    "            {\n",
    "                \"role\": <role>,\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": <image_url> },  # if applicable\n",
    "                    { \"type\": \"text\", \"text\": <text> }           # if applicable\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    The returned list begins with a hard-coded system message.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the system message per your requirements.\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a helpful deepfake detector assistant. You are at the core of an Agent System and you have several function calls at your disposal to assist with deepfake detection. Your task is to synthesize the exeprts' feedback into a coherent deepfake assessment, not to make the assessment yourself.\"\n",
    "                        \"To be clear: Your task is not to summarize the experts' feedbacks, but to synthesize their findings into a coherent, forensically verifiable format. Also, make sure to \\textbf{emphasize} important words in each sentence.\"\n",
    "                        \"The user will not provide you with the experts' feedback. The user will provide you with the video to be analyzed, and you will consult the relevant experts as needed.\"\n",
    "                        \"The experts you will be using are the state-of-the-art Deep Learning models, such as SPSL (Liu et Al., 2021) a frenquency inconsistencies expert model, UCF (Yan et Al., 2023) a spatial inconsistencies expert model, and Xception (Rossler et Al., 2019) a naïve, general inconsistencies expert model.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    formatted = [system_message]\n",
    "    \n",
    "    # Sort messages by ascending message id\n",
    "    sorted_messages = sorted(db_messages, key=lambda msg: msg[\"id\"])\n",
    "    \n",
    "    for msg in sorted_messages:\n",
    "        #print(\n",
    "        #   f\"DEBUG media_type={msg.get('media_type')} \"\n",
    "        #   f\"media_url={msg.get('media_url')} content={msg.get('content')}\"\n",
    "        #)\n",
    "    \n",
    "        entry = {\"role\": msg.get(\"role\", \"user\"), \"content\": []}\n",
    "    \n",
    "        if msg.get(\"media_type\") in [\"image\", \"video\"] and msg.get(\"media_url\"):\n",
    "            #print(\"DEBUG: Adding media block for this message!\")  # <--- Debug line\n",
    "            entry[\"content\"].append({\n",
    "                \"type\": msg[\"media_type\"],  \n",
    "                \"image\" if msg[\"media_type\"] == \"image\" else \"video\": f\"file://{msg['media_url']}\"\n",
    "            })\n",
    "        \n",
    "        # If there's text content (even if there was an image), add it as a text content block.\n",
    "        text = msg.get(\"content\", \"\").strip()\n",
    "        if text:\n",
    "            entry[\"content\"].append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": text\n",
    "            })\n",
    "        \n",
    "        # Append the formatted message to the list.\n",
    "        formatted.append(entry)\n",
    "    \n",
    "    return formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec6a2a00-a5d5-4386-87d6-c5e153046f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pprint\n",
    "from supabase_wrapper import get_all_messages, create_conversation, insert_message\n",
    "\n",
    "node_dir = \"/scratch/mmm9912/condaDONOTDELETE/qwen25vl/bin\"\n",
    "os.environ[\"PATH\"] = node_dir + os.pathsep + os.environ[\"PATH\"]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# If __file__ is not defined (e.g. in a notebook), use the current working directory.\n",
    "try:\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d59bbdd3-253b-488f-aae3-56a330c0e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial highest message id: 157\n",
      "New message detected in conversation 44.\n",
      "LLM finished answering. Sending message...\n",
      "Message sent.\n",
      "\n",
      "New message detected in conversation 44.\n",
      "LLM finished answering. Sending message...\n",
      "Message sent.\n",
      "\n",
      "New message detected in conversation 44.\n",
      "LLM finished answering. Sending message...\n",
      "Message sent.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# If the highest id has not changed, wait 1 second and continue.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_highest \u001b[38;5;241m==\u001b[39m highest_id:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Find the new message with the highest id.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the initial list of messages and determine the current highest message id.\n",
    "initial_data = get_all_messages()\n",
    "messages = initial_data.get(\"messages\", [])\n",
    "\n",
    "highest_id = max((message[\"id\"] for message in messages), default=0)  # Ensure it starts from 0 if no messages exist\n",
    "\n",
    "print(\"Initial highest message id:\", highest_id)\n",
    "\n",
    "# Infinite loop to poll for new messages.\n",
    "while True:\n",
    "    # Retrieve the entire messages table from the database.\n",
    "    data = get_all_messages()\n",
    "    messages = data.get(\"messages\", [])\n",
    "    \n",
    "    # If there are no messages, wait and continue.\n",
    "    if not messages:\n",
    "        print(\"No messages found in the database. Waiting...\")\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    # Determine the current highest message id.\n",
    "    current_highest = max(message[\"id\"] for message in messages)\n",
    "    \n",
    "    # If the highest id has not changed, wait 1 second and continue.\n",
    "    if current_highest == highest_id:\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    \n",
    "    # Find the new message with the highest id.\n",
    "    new_message = next((msg for msg in messages if msg[\"id\"] == current_highest), None)\n",
    "    if new_message is None:\n",
    "        print(\"Error: Highest id message not found. Waiting...\")\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "    \n",
    "    # Extract the conversation id from the new message.\n",
    "    conversation_id = new_message[\"conversation_id\"]\n",
    "    \n",
    "    # Filter messages that belong to this conversation.\n",
    "    conversation_messages = [msg for msg in messages if msg[\"conversation_id\"] == conversation_id]\n",
    "    \n",
    "    # Sort the messages by ascending message id.\n",
    "    conversation_messages_sorted = sorted(conversation_messages, key=lambda msg: msg[\"id\"])\n",
    "    #print(\"Conversation messages sorted:\", conversation_messages_sorted)\n",
    "    \n",
    "    print(f\"New message detected in conversation {conversation_id}.\")\n",
    "    \n",
    "    # Format the conversation history properly\n",
    "    formatted_messages = format_conversation(conversation_messages_sorted)\n",
    "    #print(\"Formatted messages:\", formatted_messages)\n",
    "    \n",
    "    # Prepare the conversation text\n",
    "    text = processor.apply_chat_template(\n",
    "        formatted_messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # **Process any images or videos in the latest message**\n",
    "    image_inputs, video_inputs = process_vision_info(formatted_messages)\n",
    "    \n",
    "    # **Create input tensor dynamically based on available media**\n",
    "    inputs = processor(\n",
    "        text=[text], \n",
    "        images=image_inputs if image_inputs else None, \n",
    "        videos=video_inputs if video_inputs else None, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # **Generate model response**\n",
    "    response_text = answer(model, processor, inputs)\n",
    "    print(\"LLM finished answering. Sending message...\")\n",
    "    #print(response_text)\n",
    "\n",
    "    # **Insert assistant response into the database**\n",
    "    insert_message(\n",
    "        conversation_id=conversation_id,  # Use same conversation ID\n",
    "        role=\"assistant\",                 # Mark response as from assistant\n",
    "        content=response_text,             # Store generated response\n",
    "        media_url=None,                    # No media in text response\n",
    "        media_type=None                    # No media type\n",
    "    )\n",
    "    print(\"Message sent.\\n\")\n",
    "\n",
    "    # **Update the highest message ID**\n",
    "    highest_id = max(highest_id, current_highest) + 1 \n",
    "    \n",
    "    # Wait before checking for new messages again\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317d1ba-8f75-41df-ab01-a2fe4c29d902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0b57b-7ad9-41b4-ba9b-699ea59f9152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c1c5a-8c1c-4f4c-83ff-83dcde34470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589e645-f639-43f0-9331-9f3dcc06f6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecd0cd-6054-42c6-8110-383430413c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ab58a-3c4f-4b43-8e77-5b4b7334893e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen25vl)",
   "language": "python",
   "name": "qwen25vl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
