{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05f55fc-3174-4094-9d0c-4c7e4a4014b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC set to: /share/apps/NYUAD5/gcc/9.2.0/bin/gcc\n",
      "CXX set to: /share/apps/NYUAD5/gcc/9.2.0/bin/g++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\n",
      "========================================\n",
      "INFO 04-15 06:04:50 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-15 06:04:58 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:04:58 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:04:58 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:04:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:00 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-15 06:05:01 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f04bc01138a4ad693871fcea96ab5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:03 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 04-15 06:05:05 worker.py:267] Memory profiling takes 1.03 seconds\n",
      "INFO 04-15 06:05:05 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:05 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 66.87GiB.\n",
      "INFO 04-15 06:05:05 executor_base.py:111] # cuda blocks: 156503, # CPU blocks: 9362\n",
      "INFO 04-15 06:05:05 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1222.68x\n",
      "INFO 04-15 06:05:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 4.73 seconds\n",
      "Error: name 'measure_inference_memory' is not defined\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\n",
      "========================================\n",
      "INFO 04-15 06:05:13 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:13 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:05:13 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:13 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:13 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:14 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce095f6e870b4a1bb839e3648cbf1102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:17 model_runner.py:1115] Loading model weights took 1.6816 GB\n",
      "INFO 04-15 06:05:18 worker.py:267] Memory profiling takes 0.57 seconds\n",
      "INFO 04-15 06:05:18 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:18 worker.py:267] model weights take 1.68GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 68.17GiB.\n",
      "INFO 04-15 06:05:19 executor_base.py:111] # cuda blocks: 159554, # CPU blocks: 9362\n",
      "INFO 04-15 06:05:19 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1246.52x\n",
      "Error: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 79.15 GiB of which 2.12 GiB is free. Including non-PyTorch memory, this process has 77.01 GiB memory in use. Of the allocated memory 76.33 GiB is allocated by PyTorch, and 183.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\n",
      "========================================\n",
      "INFO 04-15 06:05:21 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:21 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:05:21 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:21 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:21 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:22 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9ff7340a2340f6824fb94c621a3643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:24 model_runner.py:1115] Loading model weights took 1.0855 GB\n",
      "INFO 04-15 06:05:25 worker.py:267] Memory profiling takes 0.54 seconds\n",
      "INFO 04-15 06:05:25 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:25 worker.py:267] model weights take 1.09GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 68.77GiB.\n",
      "INFO 04-15 06:05:26 executor_base.py:111] # cuda blocks: 160954, # CPU blocks: 9362\n",
      "INFO 04-15 06:05:26 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1257.45x\n",
      "Error: CUDA out of memory. Tried to allocate 2.46 GiB. GPU 0 has a total capacity of 79.15 GiB of which 343.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 78.23 GiB is allocated by PyTorch, and 60.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\n",
      "========================================\n",
      "INFO 04-15 06:05:28 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:28 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:28 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:29 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4eb30baff24b78b37da08c6549992d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:33 model_runner.py:1115] Loading model weights took 5.7837 GB\n",
      "INFO 04-15 06:05:34 worker.py:267] Memory profiling takes 0.56 seconds\n",
      "INFO 04-15 06:05:34 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:34 worker.py:267] model weights take 5.78GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 64.07GiB.\n",
      "INFO 04-15 06:05:35 executor_base.py:111] # cuda blocks: 116630, # CPU blocks: 7281\n",
      "INFO 04-15 06:05:35 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 911.17x\n",
      "Error: CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 77.34 GiB is allocated by PyTorch, and 218.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\n",
      "========================================\n",
      "INFO 04-15 06:05:37 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:37 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:05:37 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:37 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:37 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:38 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc8780f471941c5b68b741944bf8893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:41 model_runner.py:1115] Loading model weights took 3.2509 GB\n",
      "INFO 04-15 06:05:42 worker.py:267] Memory profiling takes 0.57 seconds\n",
      "INFO 04-15 06:05:42 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:42 worker.py:267] model weights take 3.25GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 66.60GiB.\n",
      "INFO 04-15 06:05:43 executor_base.py:111] # cuda blocks: 121241, # CPU blocks: 7281\n",
      "INFO 04-15 06:05:43 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 947.20x\n",
      "Error: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 77.40 GiB memory in use. Of the allocated memory 76.73 GiB is allocated by PyTorch, and 173.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\n",
      "========================================\n",
      "INFO 04-15 06:05:45 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:45 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:05:45 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:45 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:45 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:46 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e0528d474642b4a0c8062b86778e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:05:49 model_runner.py:1115] Loading model weights took 1.9277 GB\n",
      "INFO 04-15 06:05:50 worker.py:267] Memory profiling takes 0.56 seconds\n",
      "INFO 04-15 06:05:50 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:05:50 worker.py:267] model weights take 1.93GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 67.92GiB.\n",
      "INFO 04-15 06:05:50 executor_base.py:111] # cuda blocks: 123649, # CPU blocks: 7281\n",
      "INFO 04-15 06:05:50 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 966.01x\n",
      "Error: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 78.00 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 141.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\n",
      "========================================\n",
      "INFO 04-15 06:05:53 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:53 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:53 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:53 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:54 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16...\n",
      "Error: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 191.25 MiB is free. Including non-PyTorch memory, this process has 78.94 GiB memory in use. Of the allocated memory 78.26 GiB is allocated by PyTorch, and 181.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\n",
      "========================================\n",
      "INFO 04-15 06:05:56 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:05:56 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:05:56 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:05:56 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:05:56 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:05:57 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8bd77f72c54f7aa12f49f28016b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:06:05 model_runner.py:1115] Loading model weights took 8.2107 GB\n",
      "Error: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 78.17 GiB is allocated by PyTorch, and 453.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\n",
      "========================================\n",
      "INFO 04-15 06:06:08 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:06:08 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:06:08 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:06:08 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:06:08 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:06:09 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c893cf63054fa9800dbd6f2d8d2d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 06:06:15 model_runner.py:1115] Loading model weights took 5.1396 GB\n",
      "INFO 04-15 06:06:16 worker.py:267] Memory profiling takes 0.64 seconds\n",
      "INFO 04-15 06:06:16 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 06:06:16 worker.py:267] model weights take 5.14GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 64.70GiB.\n",
      "INFO 04-15 06:06:17 executor_base.py:111] # cuda blocks: 75720, # CPU blocks: 4681\n",
      "INFO 04-15 06:06:17 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 591.56x\n",
      "Error: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.07 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 349.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\n",
      "========================================\n",
      "INFO 04-15 06:06:19 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:06:19 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:06:19 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:06:19 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:06:20 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16...\n",
      "Error: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 55.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 78.42 GiB is allocated by PyTorch, and 154.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\n",
      "========================================\n",
      "INFO 04-15 06:06:23 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:06:23 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:06:23 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:06:23 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:06:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:06:24 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8...\n",
      "Error: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 117.25 MiB is free. Including non-PyTorch memory, this process has 79.01 GiB memory in use. Of the allocated memory 78.40 GiB is allocated by PyTorch, and 113.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "========================================\n",
      "Processing: /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\n",
      "========================================\n",
      "INFO 04-15 06:06:27 config.py:549] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 06:06:27 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 06:06:27 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 06:06:27 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 06:06:27 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 06:06:28 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4...\n",
      "Error: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 79.15 GiB of which 923.25 MiB is free. Including non-PyTorch memory, this process has 78.22 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 139.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Memory Usage Report:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Only allow CUDA device(s); disable CPU fallback\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "\n",
    "# Optional: disable multithreaded CPU ops to avoid CPU fallback paths\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"INFERENCE_ENGINE_DISABLE_CPU\"] = \"1\"\n",
    "\n",
    "# Set all cache directories explicitly\n",
    "scratch_root = \"/scratch/mmm9912\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/scratch/mmm9912/cache/torch\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/scratch/mmm9912/cache/tensorflow\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/scratch/mmm9912/cache/huggingface_datasets\"\n",
    "os.environ[\"PIP_CACHE_DIR\"] = \"/scratch/mmm9912/cache/pip\"\n",
    "\n",
    "cache_dir = \"/scratch/mmm9912/cache\"\n",
    "\n",
    "gcc_path = \"/share/apps/NYUAD5/gcc/9.2.0/bin/gcc\"\n",
    "gcc_dir = os.path.dirname(gcc_path)\n",
    "\n",
    "os.environ[\"CC\"] = gcc_path\n",
    "os.environ[\"CXX\"] = f\"{gcc_dir}/g++\"  # Set C++ compiler as well\n",
    "\n",
    "print(\"CC set to:\", os.environ[\"CC\"])\n",
    "print(\"CXX set to:\", os.environ[\"CXX\"])\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# Blow up if any tensor goes to CPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from prompting_utils_local import *\n",
    "\n",
    "# ─── MEMORY MEASUREMENT FUNCTIONS ─────────────────────────────────────────────\n",
    "def measure_model_memory(model_path, group_size=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        # 1. Get true baseline\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        baseline = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # 2. Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=False,\n",
    "            padding_side=\"left\",\n",
    "            pad_token=\"<|endoftext|>\"\n",
    "        )\n",
    "        \n",
    "        # 3. Temporary config for pure weight measurement\n",
    "        is_quantized = any(x in model_path for x in ['int4', 'int8'])\n",
    "        llm_config = {\n",
    "            \"model\": model_path,\n",
    "            \"dtype\": \"float16\",\n",
    "            \"quantization\": \"gptq\" if is_quantized else None,\n",
    "            \"tensor_parallel_size\": 1,\n",
    "            \"trust_remote_code\": True,\n",
    "            # Set a minimal context length; caching is not required.\n",
    "            \"max_model_len\": 256,\n",
    "            \"disable_log_stats\": True\n",
    "        }\n",
    "        if is_quantized:\n",
    "            llm_config.update({\n",
    "                \"gptq_group_size\": group_size,\n",
    "                \"gptq_desc_act\": True\n",
    "            })\n",
    "        \n",
    "        # 4. Load model and measure the weight memory usage\n",
    "        llm = LLM(**llm_config)\n",
    "        static_mem = torch.cuda.memory_allocated() - baseline\n",
    "        \n",
    "        # 5. Completely disable KV caching by overriding the cache initializer.\n",
    "        # This ensures no caching memory is allocated.\n",
    "        if hasattr(llm, \"llm_engine\") and hasattr(llm.llm_engine, \"_init_cache\"):\n",
    "            llm.llm_engine._init_cache = lambda: None\n",
    "        \n",
    "        # 6. Measure full inference memory without any cache overhead.\n",
    "        peak_mem = measure_inference_memory(llm, tokenizer, group_size)\n",
    "        # With caching disabled, any difference is from inference only.\n",
    "        kv_cache_mem = peak_mem - static_mem - baseline\n",
    "        \n",
    "        # 7. Cleanup\n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            \"Model\": os.path.basename(model_path),\n",
    "            \"Weights (MB)\": static_mem / 1024**2,\n",
    "            \"KV Cache (MB)\": kv_cache_mem / 1024**2,\n",
    "            \"Total (MB)\": (static_mem + kv_cache_mem) / 1024**2\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ─── MAIN MEASUREMENT LOOP ────────────────────────────────────────────────────\n",
    "def main():\n",
    "    model_paths = [\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\",\n",
    "        \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        print(f\"\\n{'='*40}\\nProcessing: {model_path}\\n{'='*40}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer with Qwen settings\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=False,\n",
    "                padding_side=\"left\",\n",
    "                pad_token=\"<|endoftext|>\"\n",
    "            )\n",
    "            \n",
    "            # Model configuration for inference;\n",
    "            # no caching parameters are passed. Caching is disabled by patching _init_cache.\n",
    "            model_name = os.path.basename(model_path)\n",
    "            is_quantized = any(x in model_name for x in ['int4', 'int8'])\n",
    "            \n",
    "            llm_config = {\n",
    "                \"model\": model_path,\n",
    "                \"dtype\": \"float16\",\n",
    "                \"quantization\": \"gptq\" if is_quantized else None,\n",
    "                \"enforce_eager\": True,\n",
    "                \"tensor_parallel_size\": 1,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"max_model_len\": 2048,\n",
    "                \"download_dir\": cache_dir\n",
    "            }\n",
    "\n",
    "            # Load model\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            llm = LLM(**llm_config)\n",
    "            # Disable KV caching by overriding _init_cache with a no-op.\n",
    "            if hasattr(llm, \"llm_engine\") and hasattr(llm.llm_engine, \"_init_cache\"):\n",
    "                llm.llm_engine._init_cache = lambda: None\n",
    "\n",
    "            static_mem = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Measure memory usage during inference.\n",
    "            peak_mem = measure_inference_memory(llm, tokenizer)\n",
    "            dynamic_mem = peak_mem - static_mem\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Static (MB)\": static_mem / (1024 ** 2),\n",
    "                \"Dynamic (MB)\": dynamic_mem / (1024 ** 2),\n",
    "                \"Total Peak (MB)\": peak_mem / (1024 ** 2)\n",
    "            })\n",
    "            \n",
    "            del llm\n",
    "            del tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nMemory Usage Report:\")\n",
    "    print(df.to_string(index=False))\n",
    "    df.to_csv(\"memory_results.csv\", index=False)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a311a7-8d1a-4e7d-b977-df845703b2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm)",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
