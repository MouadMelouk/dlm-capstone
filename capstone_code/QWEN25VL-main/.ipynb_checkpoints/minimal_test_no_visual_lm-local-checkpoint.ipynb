{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ab0429-4b51-40c3-8c93-1bf62d25778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC set to: /share/apps/NYUAD5/gcc/9.2.0/bin/gcc\n",
      "CXX set to: /share/apps/NYUAD5/gcc/9.2.0/bin/g++\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# Set all cache directories explicitly\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/scratch/mmm9912/cache/torch\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/scratch/mmm9912/cache/tensorflow\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/scratch/mmm9912/cache/huggingface_datasets\"\n",
    "os.environ[\"PIP_CACHE_DIR\"] = \"/scratch/mmm9912/cache/pip\"\n",
    "\n",
    "cache_dir = \"/scratch/mmm9912/cache\"\n",
    "\n",
    "gcc_path = \"/share/apps/NYUAD5/gcc/9.2.0/bin/gcc\"\n",
    "gcc_dir = os.path.dirname(gcc_path)\n",
    "\n",
    "os.environ[\"CC\"] = gcc_path\n",
    "os.environ[\"CXX\"] = f\"{gcc_dir}/g++\"  # Set C++ compiler as well\n",
    "\n",
    "print(\"CC set to:\", os.environ[\"CC\"])\n",
    "print(\"CXX set to:\", os.environ[\"CXX\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40caf3b9-d51f-44ec-a786-e81a6f57bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting_utils_local import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b745e88-515e-4b97-adc9-8cfba5d7b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 07:28:39 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-14 07:28:40 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-14 07:28:48 config.py:549] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-14 07:28:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-14 07:28:51 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-14 07:28:51 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-14 07:28:51 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "INFO 04-14 07:28:52 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6701aaecab4ee58faa18781ea3fcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 07:29:44 model_runner.py:1115] Loading model weights took 14.2487 GB\n",
      "INFO 04-14 07:29:51 worker.py:267] Memory profiling takes 6.69 seconds\n",
      "INFO 04-14 07:29:51 worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.90) = 28.57GiB\n",
      "INFO 04-14 07:29:51 worker.py:267] model weights take 14.25GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 9.88GiB.\n",
      "INFO 04-14 07:29:52 executor_base.py:111] # cuda blocks: 11558, # CPU blocks: 4681\n",
      "INFO 04-14 07:29:52 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 5.64x\n",
      "INFO 04-14 07:29:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:24<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 07:30:21 model_runner.py:1562] Graph capturing finished in 24 secs, took 0.80 GiB\n",
      "INFO 04-14 07:30:21 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 36.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", cache_dir=cache_dir)\n",
    "\n",
    "# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "\n",
    "# Input the model name or path. Can be GPTQ or AWQ models.\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    download_dir=cache_dir,\n",
    "    dtype=\"float16\"  # or dtype=\"half\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa59eb5-f43f-4319-ae0e-ee22ec5ab4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    user_message_str = (\n",
    "        \"Hi, I'd like to see if this video might be deepfaked. I want a more thorough analysis with 16 frames. \"\n",
    "        \"Use the 'naive expert' please. Here's the path: \"\n",
    "        \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/videos/1ef6e158-9f69-4100-b154-a27009e33d20.mp4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc82a3-bca7-478b-95a0-c3919512383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_single_inference] Generating initial LLM response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it, est. speed input: 235.78 toks/s, output: 38.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_single_inference] LLM response received.\n",
      "[run_single_inference] Parsing and handling LLM output...\n",
      "[consult_expert_model] Starting expert analysis with model 'xception'.\n",
      "[consult_expert_model] Storing Grad-CAM overlays in: /scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/storage/gradcams/23eb8315\n",
      "[consult_expert_model] Number of frames extracted: 16\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_gradcams(gradcam_info_list):\n",
    "    \"\"\"\n",
    "    Plots Grad-CAM overlays in a grid if there are 4 or 16 images.\n",
    "    Overlays each image with:\n",
    "      Frame: <frame_index>\n",
    "      Conf: <confidence in 2 decimals>\n",
    "      <message>\n",
    "\n",
    "    gradcam_info_list: A list of dictionaries, each with keys:\n",
    "       - 'overlay_path': str, path to the saved Grad-CAM image\n",
    "       - 'confidence': float, predicted forging confidence\n",
    "       - 'message': str, short verdict message\n",
    "       - 'frame_index': int, the extracted frame index from the video\n",
    "       ... other keys are ignored\n",
    "\n",
    "    If len(gradcam_info_list) != 4 and != 16, we print a note and do not plot in multi-grid.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(gradcam_info_list)\n",
    "    if n not in [4, 16]:\n",
    "        print(f\"[plot_gradcams] Received {n} Grad-CAM images. No 2x2 or 4x4 plot made.\")\n",
    "        return\n",
    "\n",
    "    # Determine rows/cols\n",
    "    if n == 4:\n",
    "        rows, cols = 2, 2\n",
    "        fig_size = (8, 8)\n",
    "    else:  # n == 16\n",
    "        rows, cols = 4, 4\n",
    "        fig_size = (12, 12)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=fig_size)\n",
    "    axes = axes.flatten()  # Make it a flat iterable\n",
    "\n",
    "    for ax, info in zip(axes, gradcam_info_list):\n",
    "        # Retrieve data from the dictionary\n",
    "        path = info.get(\"overlay_path\", \"\")\n",
    "        conf = info.get(\"confidence\", 0.0)\n",
    "        msg = info.get(\"message\", \"\")\n",
    "        frame_idx = info.get(\"frame_index\", -1)\n",
    "\n",
    "        # Load and display the Grad-CAM image\n",
    "        img = mpimg.imread(path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Build the label text in multiple lines\n",
    "        # e.g.:\n",
    "        #   Frame: 25\n",
    "        #   Conf: 36.61\n",
    "        #   UCF model did not detect forgery.\n",
    "        label_text = (\n",
    "            f\"Frame: {frame_idx}\\n\"\n",
    "            f\"Conf: {conf:.2f}\\n\"\n",
    "            f\"{msg}\"\n",
    "        )\n",
    "\n",
    "        # Place a text box in the lower-right corner\n",
    "        ax.text(\n",
    "            0.95, 0.05, label_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_single_inference():\n",
    "    \"\"\"\n",
    "    Runs a single inference cycle with:\n",
    "      - A hardcoded system prompt containing deepfake-detection instructions.\n",
    "      - A hardcoded user message describing the user's query.\n",
    "      - One pass of LLM generation + analysis.\n",
    "      - If needed, runs the expert model (consult_expert_model), which now returns:\n",
    "          [summary_string, dict_for_frame_1, dict_for_frame_2, ...]\n",
    "      - The final LLM pass occurs after we have the expert feedback.\n",
    "      - Prints the final LLM output, then plots Grad-CAM overlays\n",
    "        if there are exactly 4 or 16 of them, using the\n",
    "        'frame_index', 'confidence', and 'message' fields in each dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hardcoded system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, \"\n",
    "            \"markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis.\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they \"\n",
    "            \"   would like to use before proceeding.\\n\"\n",
    "            \"3.1 Only ever populate one of the two fields: direct_answer_to_frontend or consult_expert_model. \"\n",
    "            \"   Never populate both.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output \"\n",
    "            \"   the JSON object as specified and wait for the expert's response. Once you receive the expert's response, \"\n",
    "            \"   synthesize the information and present it inside direct_answer_to_frontend.\\n\"\n",
    "            \"   Answer key: 0-60% confidence is low, 61-75% is weak, 76-85% is medium, 86-100% is strong.\\n\"\n",
    "            \"Finally, the user does not understand model names. The user only understands 'frequency expert', \"\n",
    "            \"'spatial expert', 'naïve detector'. After introducing yourself, always explicitly ask the user to \"\n",
    "            \"upload a video, then to choose among these detectors.\\n\"\n",
    "            \"If they ask for model details, supply them: frequency=SPSL, spatial=UCF, naïve=Xception. \"\n",
    "            \"You only serve to direct manual forensic verification using XAI principles.\\n\\n\"\n",
    "            \"Follow these rules precisely.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Hardcoded user message\n",
    "    user_message_str = (\n",
    "        \"Hi, I'd like to see if this video might be deepfaked. I want a more thorough analysis with 16 frames. \"\n",
    "        \"Use the 'naive expert' please. Here's the path: \"\n",
    "        \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/videos/1ef6e158-9f69-4100-b154-a27009e33d20.mp4\"\n",
    "    )\n",
    "    user_message = format_user_or_assistant_message(\"user\", user_message_str)\n",
    "\n",
    "    # Build the message array for the LLM\n",
    "    messages = [system_message, user_message]\n",
    "\n",
    "    # STEP 1: Generate the initial LLM response\n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"[run_single_inference] Generating initial LLM response...\")\n",
    "    outputs = llm.generate([text_input], sampling_params)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    print(\"[run_single_inference] LLM response received.\")\n",
    "\n",
    "    # STEP 2: Process the LLM response\n",
    "    print(\"[run_single_inference] Parsing and handling LLM output...\")\n",
    "    handle_result = handle_llm_response(generated_text)\n",
    "\n",
    "    # STEP 3: Determine if we have a direct answer or an expert analysis result\n",
    "    final_answer = \"\"\n",
    "    gradcam_info_list = []\n",
    "\n",
    "    if isinstance(handle_result, str):\n",
    "        # Direct answer\n",
    "        final_answer = handle_result\n",
    "        print(\"[run_single_inference] LLM provided a direct answer with no expert model.\")\n",
    "    else:\n",
    "        # handle_result => [summary_string, dict_1, dict_2, ...]\n",
    "        # The first item is the summary; the rest are frame dictionaries with overlay info.\n",
    "        expert_feedback = handle_result[0]\n",
    "        print(\"[run_single_inference] Received expert feedback. Adding as system message...\")\n",
    "\n",
    "        # Provide the expert feedback back to the LLM (2nd pass)\n",
    "        messages.append(format_expert_message(expert_feedback))\n",
    "        text_input = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        print(\"[run_single_inference] Generating final LLM response after expert feedback...\")\n",
    "        outputs = llm.generate([text_input], sampling_params)\n",
    "        final_text = outputs[0].outputs[0].text\n",
    "        final_result = handle_llm_response(final_text)\n",
    "\n",
    "        if isinstance(final_result, str):\n",
    "            final_answer = final_result\n",
    "        else:\n",
    "            # In unusual cases, you might get another chain. We'll assume only the first item is the final summary.\n",
    "            final_answer = final_result[0]\n",
    "\n",
    "        # Now gather the gradcam dictionaries: each entry includes frame index, overlay path, confidence, etc.\n",
    "        gradcam_dicts = handle_result[1:]\n",
    "        for entry in gradcam_dicts:\n",
    "            # e.g. {\"frame_index\": 25, \"overlay_path\": \"...\", \"confidence\": 36.61, \"message\":\"...\"}\n",
    "            gradcam_info_list.append(entry)\n",
    "\n",
    "        print(f\"[run_single_inference] Expert feedback indicates {len(gradcam_info_list)} Grad-CAM images.\")\n",
    "\n",
    "    # STEP 4: Final LLM answer, removing any leftover \"Uploaded media: video\" text\n",
    "    if \"Uploaded media: video\" in final_answer:\n",
    "        final_answer = final_answer.split(\"Uploaded media: video\")[0].strip()\n",
    "\n",
    "    print(\"\\n[run_single_inference] FINAL LLM ANSWER:\\n\", final_answer)\n",
    "\n",
    "    # STEP 5: Plot Grad-CAM overlays if we have 4 or 16\n",
    "    if gradcam_info_list:\n",
    "        plot_gradcams(gradcam_info_list)\n",
    "    else:\n",
    "        print(\"[run_single_inference] No Grad-CAM images to display (none returned).\")\n",
    "\n",
    "\n",
    "# Execute a single inference pass\n",
    "run_single_inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1ec0-c5fd-440d-a7f6-c3964e837ffa",
   "metadata": {},
   "source": [
    "# Example (Actual, it happened) Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909295ea-7b7e-4563-af87-162b999814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those mentioning a video or requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis (16 is the maximum allowed).\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they would like to use before proceeding.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output the JSON object as specified and wait for the expert's response. Once you receive the expert's response, synthesize the information and present it inside direct_answer_to_frontend which is the user. \"\n",
    "            \"Finally, the user does not understand the specific model names. The user only understands the terms 'frequency expert', 'spatial expert', 'naïve detector'. Always explicitly ask the user to choose among these detectors. \"\n",
    "            \"However, if the user asks for the specific detectors' details, then supply them with the information: frequency is SPSL (Honggu et Al., 2021), spatial is UCF (Zhiyuan et Al., 2023), naïve is Xception (Rossler et Al., 2019). One last thing: You only serve to direct manual forensic verification using the principles of Explainable AI (XAI), you do not replace it. If needed, reiterate that you only use principles of XAI, but manual forensic verification is needed for a definitive conclusion.\\n\\n\"\n",
    "            \"Follow these rules precisely.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"I am an AI system designed to assist with deepfake detection tasks.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Well, yeah, but what exactly do you do? What is the range of your possibilities?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"I assist with deepfake detection tasks by analyzing videos for inconsistencies using expert models. I can detect frequency, spatial, and general inconsistencies in videos.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Is the following video a deepfake? '/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4'\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"xception\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": 4\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by XCEPTION were 0, 177, 355, 532. In frame 0 forgery was detected with confidence 75.0%, \"\n",
    "            \"in frame 177 forgery was detected with confidence 75.0%, in frame 355 forgery was detected with confidence 75.0%, \"\n",
    "            \"and in frame 532 forgery was detected with confidence 75.0%. The average confidence was 75.0%. The areas that potentially \"\n",
    "            \"contain adversarial artifacts comprise of 10.0%, 10.0%, 10.0%, and 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The video at '/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4' is likely a deepfake. \"\n",
    "            \"The Xception model detected forgery with an average confidence of 75.0%. Adversarial artifacts were found in approximately \"\n",
    "            \"10.0% of each analyzed frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Amazing! What did this expert detector look at?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The Xception model, also known as the naïve detector, analyzed the video for general inconsistencies. It looks at various features in the frames to detect signs of forgery,\"\n",
    "            \" such as unusual patterns, artifacts, and other anomalies that are not typically present in authentic videos.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Do a more in-depth analysis.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by XCEPTION were 0, 44, 88, 133, 177, 221, 266, 310, 355, 399, 443, 488, 532, 576, 621, 665.\"\n",
    "            \"In frame 0 forgery was detected with confidence 75.0%, In frame 44 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 88 forgery was detected with confidence 75.0%, In frame 133 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 177 forgery was detected with confidence 75.0%, In frame 221 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 266 forgery was detected with confidence 75.0%, In frame 310 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 355 forgery was detected with confidence 75.0%, In frame 399 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 443 forgery was detected with confidence 75.0%, In frame 488 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 532 forgery was detected with confidence 75.0%, In frame 576 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 621 forgery was detected with confidence 75.0%, In frame 665 forgery was detected with confidence 75.0%. \"\n",
    "            \"The average confidence was 75.0%. The areas that potentially contain adversarial artifacts comprise of 10.0%, 10.0%, \"\n",
    "            \"10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The more in-depth analysis using the Xception model confirmed that the video is likely \"\n",
    "            \"a deepfake. Forgery was detected in all 16 analyzed frames with an average confidence of 75.0%. Adversarial artifacts were \"\n",
    "            \"found in approximately 10.0% of each frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Very good. Now what other experts do you have at your disposal?.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"In addition to the Xception model (naïve detector), I have access to two other expert \"\n",
    "            \"models for deepfake detection:\\\\n- The frequency expert (SPSL) which detects inconsistencies in the frequency domain.\\\\n- \"\n",
    "            \"The spatial expert (UCF) which identifies spatial inconsistencies in the video frames.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Let's see what the spatial expert says.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by UCF were 0, 177, 355, 532. In frame 0 forgery was detected with confidence 75.0%, In frame 177 \"\n",
    "            \"forgery was detected with confidence 75.0%, In frame 355 forgery was detected with confidence 75.0%, In frame 532 forgery \"\n",
    "            \"was detected with confidence 75.0%. The average confidence was 75.0%. The areas that potentially contain adversarial \"\n",
    "            \"artifacts comprise of 10.0%, 10.0%, 10.0%, 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The spatial expert (UCF) also detected forgery in the video with an average confidence\"\n",
    "            \"of 75.0%. Adversarial artifacts were found in approximately 10.0% of each analyzed frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Which specific frames did UCF analyze, and what did it find? What do you overall, can I for sure say this video is a deepfake?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm)",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
