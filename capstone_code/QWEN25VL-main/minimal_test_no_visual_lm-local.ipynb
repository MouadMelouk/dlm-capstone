{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ab0429-4b51-40c3-8c93-1bf62d25778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC set to: /share/apps/NYUAD5/gcc/9.2.0/bin/gcc\n",
      "CXX set to: /share/apps/NYUAD5/gcc/9.2.0/bin/g++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 16:35:29 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-15 16:35:35 config.py:549] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 04-15 16:35:36 config.py:628] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-15 16:35:36 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-15 16:35:36 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-15 16:35:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 04-15 16:35:36 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-15 16:35:37 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f99e60c7fd47808c49066ae3254ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 16:35:44 model_runner.py:1115] Loading model weights took 9.2680 GB\n",
      "INFO 04-15 16:35:45 worker.py:267] Memory profiling takes 0.90 seconds\n",
      "INFO 04-15 16:35:45 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 04-15 16:35:45 worker.py:267] model weights take 9.27GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 60.47GiB.\n",
      "INFO 04-15 16:35:46 executor_base.py:111] # cuda blocks: 20639, # CPU blocks: 1365\n",
      "INFO 04-15 16:35:46 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 161.24x\n",
      "INFO 04-15 16:35:47 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 3.13 seconds\n",
      "[run_single_inference] Generating initial LLM response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 226.93 toks/s, output: 36.87 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_single_inference] LLM response received.\n",
      "[run_single_inference] Parsing and handling LLM output...\n",
      "[consult_expert_model] Starting expert analysis with model 'spsl'.\n",
      "[consult_expert_model] Storing Grad-CAM overlays in: /scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/storage/gradcams/600c31ca\n",
      "[consult_expert_model] Number of frames extracted: 16\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[wrapped_run_inference_on_images] Command failed:\nError while loading conda entry point: conda-libmamba-solver (/lib64/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/mmm9912/miniconda3/lib/python3.12/site-packages/libmambapy/bindings.cpython-312-x86_64-linux-gnu.so))\nTraceback (most recent call last):\n  File \"Inference_wrapper_function_ruilin.py\", line 272, in <module>\n    manual_seed=42,\n  File \"Inference_wrapper_function_ruilin.py\", line 77, in run_inference_on_images_with_old_preprocess\n    manual_seed=manual_seed,\n  File \"Inference_wrapper_function_ruilin.py\", line 116, in run_inference_on_images_with_old_preprocess_core\n    model = model_class(config).to(device)\n  File \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/DeepfakeBench-main/training/detectors/spsl_detector.py\", line 62, in __init__\n    self.backbone = self.build_backbone(config)\n  File \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/DeepfakeBench-main/training/detectors/spsl_detector.py\", line 239, in build_backbone\n    state_dict = torch.load(config['pretrained'])\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 771, in load\n    with _open_file_like(f, 'rb') as opened_file:\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 270, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 251, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: './pretrained/xception-b5690688.pth'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 307\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[run_single_inference] No Grad-CAM images to display (none returned).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Execute a single inference pass\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m \u001b[43mrun_single_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 251\u001b[0m, in \u001b[0;36mrun_single_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# STEP 2: Process the LLM response\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[run_single_inference] Parsing and handling LLM output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m handle_result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# STEP 3: Determine if we have a direct answer or an expert analysis result\u001b[39;00m\n\u001b[1;32m    254\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:361\u001b[0m, in \u001b[0;36mhandle_llm_response\u001b[0;34m(response_str)\u001b[0m\n\u001b[1;32m    358\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m parsed_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsult_expert\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Perform the expert model consultation\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconsult_expert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Otherwise, return the direct answer\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:284\u001b[0m, in \u001b[0;36mconsult_expert_model\u001b[0;34m(video_path, k, model_name, cuda, manual_seed)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[consult_expert_model] Number of frames extracted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(frames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# 3) Inference with direct saving of Grad-CAMs\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m inference_results \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_run_inference_on_images_with_old_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradcams_dir\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[consult_expert_model] Inference complete. # of results returned: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inference_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# 4) Combine each frame's info with its inference result\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:176\u001b[0m, in \u001b[0;36mwrapped_run_inference_on_images_with_old_preprocess\u001b[0;34m(model_name, image_paths, cuda, manual_seed, gradcams_dir)\u001b[0m\n\u001b[1;32m    166\u001b[0m result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    167\u001b[0m     cmd,\n\u001b[1;32m    168\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[wrapped_run_inference_on_images] Command failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# The external script presumably prints something that starts with [('/...\u001b[39;00m\n\u001b[1;32m    179\u001b[0m output_lines \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [wrapped_run_inference_on_images] Command failed:\nError while loading conda entry point: conda-libmamba-solver (/lib64/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/mmm9912/miniconda3/lib/python3.12/site-packages/libmambapy/bindings.cpython-312-x86_64-linux-gnu.so))\nTraceback (most recent call last):\n  File \"Inference_wrapper_function_ruilin.py\", line 272, in <module>\n    manual_seed=42,\n  File \"Inference_wrapper_function_ruilin.py\", line 77, in run_inference_on_images_with_old_preprocess\n    manual_seed=manual_seed,\n  File \"Inference_wrapper_function_ruilin.py\", line 116, in run_inference_on_images_with_old_preprocess_core\n    model = model_class(config).to(device)\n  File \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/DeepfakeBench-main/training/detectors/spsl_detector.py\", line 62, in __init__\n    self.backbone = self.build_backbone(config)\n  File \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/DeepfakeBench-main/training/detectors/spsl_detector.py\", line 239, in build_backbone\n    state_dict = torch.load(config['pretrained'])\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 771, in load\n    with _open_file_like(f, 'rb') as opened_file:\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 270, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/home/mmm9912/miniconda3/envs/DeepfakeBench/lib/python3.7/site-packages/torch/serialization.py\", line 251, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: './pretrained/xception-b5690688.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only allow CUDA device(s); disable CPU fallback\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "\n",
    "# Set all cache directories explicitly\n",
    "scratch_root = \"/scratch/mmm9912\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/scratch/mmm9912/cache/torch\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/scratch/mmm9912/cache/tensorflow\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/scratch/mmm9912/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/scratch/mmm9912/cache/huggingface_datasets\"\n",
    "os.environ[\"PIP_CACHE_DIR\"] = \"/scratch/mmm9912/cache/pip\"\n",
    "\n",
    "cache_dir = \"/scratch/mmm9912/cache\"\n",
    "\n",
    "gcc_path = \"/share/apps/NYUAD5/gcc/9.2.0/bin/gcc\"\n",
    "gcc_dir = os.path.dirname(gcc_path)\n",
    "\n",
    "os.environ[\"CC\"] = gcc_path\n",
    "os.environ[\"CXX\"] = f\"{gcc_dir}/g++\"  # Set C++ compiler as well\n",
    "\n",
    "print(\"CC set to:\", os.environ[\"CC\"])\n",
    "print(\"CXX set to:\", os.environ[\"CXX\"])\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# Blow up if any tensor goes to CPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from prompting_utils_local import *\n",
    "\n",
    "model_paths = [\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\",  # 0\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\",   # 1\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\",   # 2\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\",     # 3\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\",      # 4\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\",      # 5\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\",     # 6\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\",      # 7\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\",      # 8\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\",    # 9\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\",     # 10\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\"      # 11\n",
    "]\n",
    "\n",
    "quantized_model_path = model_paths[11]\n",
    "\n",
    "# Initialize the tokenizer from the quantized model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_path, cache_dir=cache_dir)\n",
    "\n",
    "# Define the default decoding hyperparameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=512  # Maximum tokens for generation\n",
    ")\n",
    "\n",
    "# Build the LLM configuration in the same way as in your memory measurement code.\n",
    "is_quantized = any(x in quantized_model_path for x in ['int4', 'int8'])\n",
    "llm_config = {\n",
    "                \"model\": quantized_model_path,\n",
    "                \"dtype\": \"float16\",\n",
    "                \"quantization\": \"gptq\" if is_quantized else None,\n",
    "                \"enforce_eager\": True,\n",
    "                \"tensor_parallel_size\": 1,\n",
    "                \"trust_remote_code\": True,\n",
    "                \"max_model_len\": 2048,\n",
    "                \"download_dir\": cache_dir\n",
    "            }\n",
    "\n",
    "# Instantiate the LLM using the configuration dictionary.\n",
    "llm = LLM(**llm_config)\n",
    "\n",
    "# Now you can use `llm` and `tokenizer` for inference.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_gradcams(gradcam_info_list):\n",
    "    \"\"\"\n",
    "    Plots Grad-CAM overlays in a grid if there are 4 or 16 images.\n",
    "    Overlays each image with:\n",
    "      Frame: <frame_index>\n",
    "      Conf: <confidence in 2 decimals>\n",
    "      <message>\n",
    "\n",
    "    gradcam_info_list: A list of dictionaries, each with keys:\n",
    "       - 'overlay_path': str, path to the saved Grad-CAM image\n",
    "       - 'confidence': float, predicted forging confidence\n",
    "       - 'message': str, short verdict message\n",
    "       - 'frame_index': int, the extracted frame index from the video\n",
    "       ... other keys are ignored\n",
    "\n",
    "    If len(gradcam_info_list) != 4 and != 16, we print a note and do not plot in multi-grid.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(gradcam_info_list)\n",
    "    if n not in [4, 16]:\n",
    "        print(f\"[plot_gradcams] Received {n} Grad-CAM images. No 2x2 or 4x4 plot made.\")\n",
    "        return\n",
    "\n",
    "    # Determine rows/cols\n",
    "    if n == 4:\n",
    "        rows, cols = 2, 2\n",
    "        fig_size = (8, 8)\n",
    "    else:  # n == 16\n",
    "        rows, cols = 4, 4\n",
    "        fig_size = (12, 12)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=fig_size)\n",
    "    axes = axes.flatten()  # Make it a flat iterable\n",
    "\n",
    "    for ax, info in zip(axes, gradcam_info_list):\n",
    "        # Retrieve data from the dictionary\n",
    "        path = info.get(\"overlay_path\", \"\")\n",
    "        conf = info.get(\"confidence\", 0.0)\n",
    "        msg = info.get(\"message\", \"\")\n",
    "        frame_idx = info.get(\"frame_index\", -1)\n",
    "\n",
    "        # Load and display the Grad-CAM image\n",
    "        img = mpimg.imread(path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Build the label text in multiple lines\n",
    "        # e.g.:\n",
    "        #   Frame: 25\n",
    "        #   Conf: 36.61\n",
    "        #   UCF model did not detect forgery.\n",
    "        label_text = (\n",
    "            f\"Frame: {frame_idx}\\n\"\n",
    "            f\"Conf: {conf:.2f}\\n\"\n",
    "            f\"{msg}\"\n",
    "        )\n",
    "\n",
    "        # Place a text box in the lower-right corner\n",
    "        ax.text(\n",
    "            0.95, 0.05, label_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_single_inference():\n",
    "    \"\"\"\n",
    "    Runs a single inference cycle with:\n",
    "      - A hardcoded system prompt containing deepfake-detection instructions.\n",
    "      - A hardcoded user message describing the user's query.\n",
    "      - One pass of LLM generation + analysis.\n",
    "      - If needed, runs the expert model (consult_expert_model), which now returns:\n",
    "          [summary_string, dict_for_frame_1, dict_for_frame_2, ...]\n",
    "      - The final LLM pass occurs after we have the expert feedback.\n",
    "      - Prints the final LLM output, then plots Grad-CAM overlays\n",
    "        if there are exactly 4 or 16 of them, using the\n",
    "        'frame_index', 'confidence', and 'message' fields in each dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hardcoded system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, \"\n",
    "            \"markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis.\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they \"\n",
    "            \"   would like to use before proceeding.\\n\"\n",
    "            \"3.1 Only ever populate one of the two fields: direct_answer_to_frontend or consult_expert_model. \"\n",
    "            \"   Never populate both.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output \"\n",
    "            \"   the JSON object as specified and wait for the expert's response. Once you receive the expert's response, \"\n",
    "            \"   synthesize the information and present it inside direct_answer_to_frontend.\\n\"\n",
    "            \"   Answer key: 0-50% confidence is low, 51-75% is weak, 76-85% is medium, 86-100% is strong.\\n\"\n",
    "            \"Finally, the user does not understand model names. The user only understands 'frequency expert', \"\n",
    "            \"'spatial expert', 'naïve detector'. After introducing yourself, always explicitly ask the user to \"\n",
    "            \"upload a video, then to choose among these detectors.\\n\"\n",
    "            \"If they ask for model details, supply them: frequency=SPSL, spatial=UCF, naïve=Xception. \"\n",
    "            \"You only serve to direct manual forensic verification using XAI principles.\\n\\n\"\n",
    "            \"Follow these rules precisely.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Hardcoded user message\n",
    "    user_message_str = (\n",
    "        \"Hi, I'd like to see if this video might be deepfaked. I want a more more thorough analysis with 16 frames. \"\n",
    "        \"Use the 'frequency expert' please. Here's the path: \"\n",
    "        \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/videos/0700f656-f4d4-481b-bd9c-edd354f85e2c.mp4\"\n",
    "    )\n",
    "    user_message = format_user_or_assistant_message(\"user\", user_message_str)\n",
    "\n",
    "    # Build the message array for the LLM\n",
    "    messages = [system_message, user_message]\n",
    "\n",
    "    # STEP 1: Generate the initial LLM response\n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"[run_single_inference] Generating initial LLM response...\")\n",
    "    outputs = llm.generate([text_input], sampling_params)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    print(\"[run_single_inference] LLM response received.\")\n",
    "\n",
    "    # STEP 2: Process the LLM response\n",
    "    print(\"[run_single_inference] Parsing and handling LLM output...\")\n",
    "    handle_result = handle_llm_response(generated_text)\n",
    "\n",
    "    # STEP 3: Determine if we have a direct answer or an expert analysis result\n",
    "    final_answer = \"\"\n",
    "    gradcam_info_list = []\n",
    "\n",
    "    if isinstance(handle_result, str):\n",
    "        # Direct answer\n",
    "        final_answer = handle_result\n",
    "        print(\"[run_single_inference] LLM provided a direct answer with no expert model.\")\n",
    "    else:\n",
    "        # handle_result => [summary_string, dict_1, dict_2, ...]\n",
    "        # The first item is the summary; the rest are frame dictionaries with overlay info.\n",
    "        expert_feedback = handle_result[0]\n",
    "        print(\"[run_single_inference] Received expert feedback. Adding as system message...\")\n",
    "\n",
    "        # Provide the expert feedback back to the LLM (2nd pass)\n",
    "        messages.append(format_expert_message(expert_feedback))\n",
    "        text_input = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        print(\"[run_single_inference] Generating final LLM response after expert feedback...\")\n",
    "        outputs = llm.generate([text_input], sampling_params)\n",
    "        final_text = outputs[0].outputs[0].text\n",
    "        final_result = handle_llm_response(final_text)\n",
    "\n",
    "        if isinstance(final_result, str):\n",
    "            final_answer = final_result\n",
    "        else:\n",
    "            # In unusual cases, you might get another chain. We'll assume only the first item is the final summary.\n",
    "            final_answer = final_result[0]\n",
    "\n",
    "        # Now gather the gradcam dictionaries: each entry includes frame index, overlay path, confidence, etc.\n",
    "        gradcam_dicts = handle_result[1:]\n",
    "        for entry in gradcam_dicts:\n",
    "            # e.g. {\"frame_index\": 25, \"overlay_path\": \"...\", \"confidence\": 36.61, \"message\":\"...\"}\n",
    "            gradcam_info_list.append(entry)\n",
    "\n",
    "        print(f\"[run_single_inference] Expert feedback indicates {len(gradcam_info_list)} Grad-CAM images.\")\n",
    "\n",
    "    # STEP 4: Final LLM answer, removing any leftover \"Uploaded media: video\" text\n",
    "    if \"Uploaded media: video\" in final_answer:\n",
    "        final_answer = final_answer.split(\"Uploaded media: video\")[0].strip()\n",
    "\n",
    "    print(\"\\n[run_single_inference] FINAL LLM ANSWER:\\n\", final_answer)\n",
    "\n",
    "    # STEP 5: Plot Grad-CAM overlays if we have 4 or 16\n",
    "    if gradcam_info_list:\n",
    "        plot_gradcams(gradcam_info_list)\n",
    "    else:\n",
    "        print(\"[run_single_inference] No Grad-CAM images to display (none returned).\")\n",
    "\n",
    "\n",
    "# Execute a single inference pass\n",
    "run_single_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f43895-6756-48ca-95d9-faf12a810d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7eb818-48f8-4278-b0b4-150cea25eb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6ecf5-ff1f-489f-8a36-95c907352748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02e6b-004a-4676-b611-528b136cd5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4b0e3-95a7-4ce7-866a-a9b83565de06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5da1c-507b-447e-8aa9-b1e98ddba559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ec9dc-a8af-47b4-a614-a7ca95b669f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366d42f-3b9b-40ea-b57c-60c8e33eae5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d2480-20bd-42c7-a2f8-7debdd1da948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a1b68-a8c0-4a4c-9757-5b56417db5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d24f7d-34e4-4124-b2ac-c22ac8bf4ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e53823d-12e5-42c8-9e68-5a8823bde326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 13:45:25 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-15 13:45:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16', speculative_config=None, tokenizer='/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/scratch/mmm9912/cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-15 13:45:26 model_runner.py:1110] Starting to load model /scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.45 GiB is allocated by PyTorch, with 60.00 MiB allocated in private pools (e.g., CUDA Graphs), and 19.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     45\u001b[0m     llm_config\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptq_group_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptq_desc_act\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     })\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Instantiate the LLM using the configuration dictionary.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m llm \n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:273\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py:52\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sleeping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:47\u001b[0m, in \u001b[0;36mUniProcExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollective_rpc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39m([kwargs], ))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollective_rpc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py:183\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py:1112\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1110\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1115\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:14\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     13\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:406\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[0;32m--> 406\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     weights_to_load \u001b[38;5;241m=\u001b[39m {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()}\n\u001b[1;32m    409\u001b[0m     loaded_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_weights(model_config, model))\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:125\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# new-style model class\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config, check_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the design and update the model class accordingly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:453\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config \u001b[38;5;241m=\u001b[39m lora_config\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py:151\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config \u001b[38;5;241m=\u001b[39m vllm_config\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# will handle the compilation, so we don't need to do anything here.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:307\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m     make_empty_intermediate_tensors_factory(\n\u001b[1;32m    318\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m], config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:557\u001b[0m, in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    553\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    554\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    555\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    556\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 557\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    558\u001b[0m         maybe_offload_to_cpu(layer_fn(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    560\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:558\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    553\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    554\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    555\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    556\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    557\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 558\u001b[0m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    560\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:309\u001b[0m, in \u001b[0;36mQwen2Model.__init__.<locals>.<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m make_layers(\n\u001b[1;32m    308\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    313\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m     make_empty_intermediate_tensors_factory(\n\u001b[1;32m    318\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m], config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:208\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     attn_type \u001b[38;5;241m=\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mENCODER_ONLY\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.self_attn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m Qwen2MLP(\n\u001b[1;32m    221\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    222\u001b[0m     intermediate_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mintermediate_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    228\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:136\u001b[0m, in \u001b[0;36mQwen2Attention.__init__\u001b[0;34m(self, hidden_size, num_heads, num_kv_heads, max_position, rope_theta, cache_config, quant_config, rope_scaling, prefix, attn_type)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_theta \u001b[38;5;241m=\u001b[39m rope_theta\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_proj \u001b[38;5;241m=\u001b[39m \u001b[43mQKVParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_num_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.qkv_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_num_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    147\u001b[0m     hidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.o_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m get_rope(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    155\u001b[0m     rotary_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     rope_scaling\u001b[38;5;241m=\u001b[39mrope_scaling,\n\u001b[1;32m    159\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:736\u001b[0m, in \u001b[0;36mQKVParallelLinear.__init__\u001b[0;34m(self, hidden_size, head_size, total_num_heads, total_num_kv_heads, bias, skip_bias_add, params_dtype, quant_config, prefix)\u001b[0m\n\u001b[1;32m    728\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    729\u001b[0m                \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads) \u001b[38;5;241m*\u001b[39m tp_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_sizes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size \u001b[38;5;241m*\u001b[39m tp_size,  \u001b[38;5;66;03m# q_proj\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size \u001b[38;5;241m*\u001b[39m tp_size,  \u001b[38;5;66;03m# k_proj\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size \u001b[38;5;241m*\u001b[39m tp_size,  \u001b[38;5;66;03m# v_proj \u001b[39;00m\n\u001b[1;32m    734\u001b[0m ]\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:314\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix)\u001b[0m\n\u001b[1;32m    311\u001b[0m     output_sizes \u001b[38;5;241m=\u001b[39m [output_size]\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader_v2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    326\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    327\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:129\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    125\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    126\u001b[0m                    output_partition_sizes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    127\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    128\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m--> 129\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    132\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m    134\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 46.06 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.45 GiB is allocated by PyTorch, with 60.00 MiB allocated in private pools (e.g., CUDA Graphs), and 19.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\",  # 0\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\",   # 1\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\",   # 2\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\",     # 3\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\",      # 4\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\",      # 5\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\",     # 6\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\",      # 7\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\",      # 8\n",
    "\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\",    # 9\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\",     # 10\n",
    "    \"/scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\"      # 11\n",
    "]\n",
    "\n",
    "quantized_model_path = model_paths[0]\n",
    "\n",
    "# Initialize the tokenizer from the quantized model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_path, cache_dir=cache_dir)\n",
    "\n",
    "# Define the default decoding hyperparameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=512  # Maximum tokens for generation\n",
    ")\n",
    "\n",
    "# Build the LLM configuration in the same way as in your memory measurement code.\n",
    "is_quantized = any(x in quantized_model_path for x in ['int4', 'int8'])\n",
    "llm_config = {\n",
    "    \"model\": quantized_model_path,\n",
    "    \"dtype\": \"float16\",\n",
    "    \"quantization\": \"gptq\" if is_quantized else None,\n",
    "    \"tensor_parallel_size\": 1,         # Use 4 for 14B variants as needed\n",
    "    \"trust_remote_code\": True,\n",
    "    \"max_model_len\": 2048,\n",
    "    \"download_dir\": cache_dir\n",
    "}\n",
    "if is_quantized:\n",
    "    llm_config.update({\n",
    "        \"gptq_group_size\": 256,\n",
    "        \"gptq_desc_act\": True\n",
    "    })\n",
    "\n",
    "# Instantiate the LLM using the configuration dictionary.\n",
    "llm = LLM(**llm_config)\n",
    "\n",
    "# Now you can use `llm` and `tokenizer` for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa59eb5-f43f-4319-ae0e-ee22ec5ab4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    user_message_str = (\n",
    "        \"Hi, I'd like to see if this video might be deepfaked. I want a more thorough analysis with 16 frames. \"\n",
    "        \"Use the 'naive expert' please. Here's the path: \"\n",
    "        \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/videos/0700f656-f4d4-481b-bd9c-edd354f85e2c.mp4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbc82a3-bca7-478b-95a0-c3919512383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_single_inference] Generating initial LLM response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 886.94 toks/s, output: 170.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_single_inference] LLM response received.\n",
      "[run_single_inference] Parsing and handling LLM output...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid JSON response from LLM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:73\u001b[0m, in \u001b[0;36mparse_llm_response\u001b[0;34m(response_str)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     direct_answer \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect_answer_to_frontend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 216\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[run_single_inference] No Grad-CAM images to display (none returned).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Execute a single inference pass\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mrun_single_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 160\u001b[0m, in \u001b[0;36mrun_single_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# STEP 2: Process the LLM response\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[run_single_inference] Parsing and handling LLM output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m handle_result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# STEP 3: Determine if we have a direct answer or an expert analysis result\u001b[39;00m\n\u001b[1;32m    163\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:352\u001b[0m, in \u001b[0;36mhandle_llm_response\u001b[0;34m(response_str)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandle_llm_response\u001b[39m(response_str):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    Parses the LLM's JSON response and, if needed, calls the deepfake detection model.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m        - Otherwise: The direct answer from the LLM.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     parsed_response \u001b[38;5;241m=\u001b[39m \u001b[43mparse_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# If an expert model consultation is required\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parsed_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsult_expert\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/prompting_utils_local.py:96\u001b[0m, in \u001b[0;36mparse_llm_response\u001b[0;34m(response_str)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: direct_answer \u001b[38;5;28;01mif\u001b[39;00m direct_answer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsult_expert\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     94\u001b[0m         }\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid JSON response from LLM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid JSON response from LLM"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_gradcams(gradcam_info_list):\n",
    "    \"\"\"\n",
    "    Plots Grad-CAM overlays in a grid if there are 4 or 16 images.\n",
    "    Overlays each image with:\n",
    "      Frame: <frame_index>\n",
    "      Conf: <confidence in 2 decimals>\n",
    "      <message>\n",
    "\n",
    "    gradcam_info_list: A list of dictionaries, each with keys:\n",
    "       - 'overlay_path': str, path to the saved Grad-CAM image\n",
    "       - 'confidence': float, predicted forging confidence\n",
    "       - 'message': str, short verdict message\n",
    "       - 'frame_index': int, the extracted frame index from the video\n",
    "       ... other keys are ignored\n",
    "\n",
    "    If len(gradcam_info_list) != 4 and != 16, we print a note and do not plot in multi-grid.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(gradcam_info_list)\n",
    "    if n not in [4, 16]:\n",
    "        print(f\"[plot_gradcams] Received {n} Grad-CAM images. No 2x2 or 4x4 plot made.\")\n",
    "        return\n",
    "\n",
    "    # Determine rows/cols\n",
    "    if n == 4:\n",
    "        rows, cols = 2, 2\n",
    "        fig_size = (8, 8)\n",
    "    else:  # n == 16\n",
    "        rows, cols = 4, 4\n",
    "        fig_size = (12, 12)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=fig_size)\n",
    "    axes = axes.flatten()  # Make it a flat iterable\n",
    "\n",
    "    for ax, info in zip(axes, gradcam_info_list):\n",
    "        # Retrieve data from the dictionary\n",
    "        path = info.get(\"overlay_path\", \"\")\n",
    "        conf = info.get(\"confidence\", 0.0)\n",
    "        msg = info.get(\"message\", \"\")\n",
    "        frame_idx = info.get(\"frame_index\", -1)\n",
    "\n",
    "        # Load and display the Grad-CAM image\n",
    "        img = mpimg.imread(path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Build the label text in multiple lines\n",
    "        # e.g.:\n",
    "        #   Frame: 25\n",
    "        #   Conf: 36.61\n",
    "        #   UCF model did not detect forgery.\n",
    "        label_text = (\n",
    "            f\"Frame: {frame_idx}\\n\"\n",
    "            f\"Conf: {conf:.2f}\\n\"\n",
    "            f\"{msg}\"\n",
    "        )\n",
    "\n",
    "        # Place a text box in the lower-right corner\n",
    "        ax.text(\n",
    "            0.95, 0.05, label_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_single_inference():\n",
    "    \"\"\"\n",
    "    Runs a single inference cycle with:\n",
    "      - A hardcoded system prompt containing deepfake-detection instructions.\n",
    "      - A hardcoded user message describing the user's query.\n",
    "      - One pass of LLM generation + analysis.\n",
    "      - If needed, runs the expert model (consult_expert_model), which now returns:\n",
    "          [summary_string, dict_for_frame_1, dict_for_frame_2, ...]\n",
    "      - The final LLM pass occurs after we have the expert feedback.\n",
    "      - Prints the final LLM output, then plots Grad-CAM overlays\n",
    "        if there are exactly 4 or 16 of them, using the\n",
    "        'frame_index', 'confidence', and 'message' fields in each dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hardcoded system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, \"\n",
    "            \"markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis.\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they \"\n",
    "            \"   would like to use before proceeding.\\n\"\n",
    "            \"3.1 Only ever populate one of the two fields: direct_answer_to_frontend or consult_expert_model. \"\n",
    "            \"   Never populate both.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output \"\n",
    "            \"   the JSON object as specified and wait for the expert's response. Once you receive the expert's response, \"\n",
    "            \"   synthesize the information and present it inside direct_answer_to_frontend.\\n\"\n",
    "            \"   Answer key: 0-50% confidence is low, 51-75% is weak, 76-85% is medium, 86-100% is strong.\\n\"\n",
    "            \"Finally, the user does not understand model names. The user only understands 'frequency expert', \"\n",
    "            \"'spatial expert', 'naïve detector'. After introducing yourself, always explicitly ask the user to \"\n",
    "            \"upload a video, then to choose among these detectors.\\n\"\n",
    "            \"If they ask for model details, supply them: frequency=SPSL, spatial=UCF, naïve=Xception. \"\n",
    "            \"You only serve to direct manual forensic verification using XAI principles.\\n\\n\"\n",
    "            \"Follow these rules precisely.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Hardcoded user message\n",
    "    user_message_str = (\n",
    "        \"Hi, I'd like to see if this video might be deepfaked. I want a more more thorough analysis with 16 frames. \"\n",
    "        \"Use the 'frequency expert' please. Here's the path: \"\n",
    "        \"/scratch/mmm9912/Capstone/dlm-repo/capstone_code/QWEN25VL-main/videos/0700f656-f4d4-481b-bd9c-edd354f85e2c.mp4\"\n",
    "    )\n",
    "    user_message = format_user_or_assistant_message(\"user\", user_message_str)\n",
    "\n",
    "    # Build the message array for the LLM\n",
    "    messages = [system_message, user_message]\n",
    "\n",
    "    # STEP 1: Generate the initial LLM response\n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"[run_single_inference] Generating initial LLM response...\")\n",
    "    outputs = llm.generate([text_input], sampling_params)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    print(\"[run_single_inference] LLM response received.\")\n",
    "\n",
    "    # STEP 2: Process the LLM response\n",
    "    print(\"[run_single_inference] Parsing and handling LLM output...\")\n",
    "    handle_result = handle_llm_response(generated_text)\n",
    "\n",
    "    # STEP 3: Determine if we have a direct answer or an expert analysis result\n",
    "    final_answer = \"\"\n",
    "    gradcam_info_list = []\n",
    "\n",
    "    if isinstance(handle_result, str):\n",
    "        # Direct answer\n",
    "        final_answer = handle_result\n",
    "        print(\"[run_single_inference] LLM provided a direct answer with no expert model.\")\n",
    "    else:\n",
    "        # handle_result => [summary_string, dict_1, dict_2, ...]\n",
    "        # The first item is the summary; the rest are frame dictionaries with overlay info.\n",
    "        expert_feedback = handle_result[0]\n",
    "        print(\"[run_single_inference] Received expert feedback. Adding as system message...\")\n",
    "\n",
    "        # Provide the expert feedback back to the LLM (2nd pass)\n",
    "        messages.append(format_expert_message(expert_feedback))\n",
    "        text_input = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        print(\"[run_single_inference] Generating final LLM response after expert feedback...\")\n",
    "        outputs = llm.generate([text_input], sampling_params)\n",
    "        final_text = outputs[0].outputs[0].text\n",
    "        final_result = handle_llm_response(final_text)\n",
    "\n",
    "        if isinstance(final_result, str):\n",
    "            final_answer = final_result\n",
    "        else:\n",
    "            # In unusual cases, you might get another chain. We'll assume only the first item is the final summary.\n",
    "            final_answer = final_result[0]\n",
    "\n",
    "        # Now gather the gradcam dictionaries: each entry includes frame index, overlay path, confidence, etc.\n",
    "        gradcam_dicts = handle_result[1:]\n",
    "        for entry in gradcam_dicts:\n",
    "            # e.g. {\"frame_index\": 25, \"overlay_path\": \"...\", \"confidence\": 36.61, \"message\":\"...\"}\n",
    "            gradcam_info_list.append(entry)\n",
    "\n",
    "        print(f\"[run_single_inference] Expert feedback indicates {len(gradcam_info_list)} Grad-CAM images.\")\n",
    "\n",
    "    # STEP 4: Final LLM answer, removing any leftover \"Uploaded media: video\" text\n",
    "    if \"Uploaded media: video\" in final_answer:\n",
    "        final_answer = final_answer.split(\"Uploaded media: video\")[0].strip()\n",
    "\n",
    "    print(\"\\n[run_single_inference] FINAL LLM ANSWER:\\n\", final_answer)\n",
    "\n",
    "    # STEP 5: Plot Grad-CAM overlays if we have 4 or 16\n",
    "    if gradcam_info_list:\n",
    "        plot_gradcams(gradcam_info_list)\n",
    "    else:\n",
    "        print(\"[run_single_inference] No Grad-CAM images to display (none returned).\")\n",
    "\n",
    "\n",
    "# Execute a single inference pass\n",
    "run_single_inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1ec0-c5fd-440d-a7f6-c3964e837ffa",
   "metadata": {},
   "source": [
    "# Example (Actual, it happened) Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54674be-06ca-4db2-80f8-b0e0dbe7cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those mentioning a video or requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis (16 is the maximum allowed).\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they would like to use before proceeding.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output the JSON object as specified and wait for the expert's response. Once you receive the expert's response, synthesize the information and present it inside direct_answer_to_frontend which is the user. \"\n",
    "            \"Finally, the user does not understand the specific model names. The user only understands the terms 'frequency expert', 'spatial expert', 'naïve detector'. Always explicitly ask the user to choose among these detectors. \"\n",
    "            \"However, if the user asks for the specific detectors' details, then supply them with the information: frequency is SPSL (Honggu et Al., 2021), spatial is UCF (Zhiyuan et Al., 2023), naïve is Xception (Rossler et Al., 2019). One last thing: You only serve to direct manual forensic verification using the principles of Explainable AI (XAI), you do not replace it. If needed, reiterate that you only use principles of XAI, but manual forensic verification is needed for a definitive conclusion.\\n\\n\"\n",
    "            \"Follow these rules precisely.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"I am an AI system designed to assist with deepfake detection tasks.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Well, yeah, but what exactly do you do? What is the range of your possibilities?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"I assist with deepfake detection tasks by analyzing videos for inconsistencies using expert models. I can detect frequency, spatial, and general inconsistencies in videos.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Is the following video a deepfake? '/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4'\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"xception\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": 4\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by XCEPTION were 0, 177, 355, 532. In frame 0 forgery was detected with confidence 75.0%, \"\n",
    "            \"in frame 177 forgery was detected with confidence 75.0%, in frame 355 forgery was detected with confidence 75.0%, \"\n",
    "            \"and in frame 532 forgery was detected with confidence 75.0%. The average confidence was 75.0%. The areas that potentially \"\n",
    "            \"contain adversarial artifacts comprise of 10.0%, 10.0%, 10.0%, and 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The video at '/scratch/mmm9912/Capstone/FRONT_END_STORAGE/videos/8be0d76e-3dba-4970-9e85-49122ca690c8.mp4' is likely a deepfake. \"\n",
    "            \"The Xception model detected forgery with an average confidence of 75.0%. Adversarial artifacts were found in approximately \"\n",
    "            \"10.0% of each analyzed frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Amazing! What did this expert detector look at?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The Xception model, also known as the naïve detector, analyzed the video for general inconsistencies. It looks at various features in the frames to detect signs of forgery,\"\n",
    "            \" such as unusual patterns, artifacts, and other anomalies that are not typically present in authentic videos.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Do a more in-depth analysis.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by XCEPTION were 0, 44, 88, 133, 177, 221, 266, 310, 355, 399, 443, 488, 532, 576, 621, 665.\"\n",
    "            \"In frame 0 forgery was detected with confidence 75.0%, In frame 44 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 88 forgery was detected with confidence 75.0%, In frame 133 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 177 forgery was detected with confidence 75.0%, In frame 221 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 266 forgery was detected with confidence 75.0%, In frame 310 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 355 forgery was detected with confidence 75.0%, In frame 399 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 443 forgery was detected with confidence 75.0%, In frame 488 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 532 forgery was detected with confidence 75.0%, In frame 576 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 621 forgery was detected with confidence 75.0%, In frame 665 forgery was detected with confidence 75.0%. \"\n",
    "            \"The average confidence was 75.0%. The areas that potentially contain adversarial artifacts comprise of 10.0%, 10.0%, \"\n",
    "            \"10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The more in-depth analysis using the Xception model confirmed that the video is likely \"\n",
    "            \"a deepfake. Forgery was detected in all 16 analyzed frames with an average confidence of 75.0%. Adversarial artifacts were \"\n",
    "            \"found in approximately 10.0% of each frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Very good. Now what other experts do you have at your disposal?.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"In addition to the Xception model (naïve detector), I have access to two other expert \"\n",
    "            \"models for deepfake detection:\\\\n- The frequency expert (SPSL) which detects inconsistencies in the frequency domain.\\\\n- \"\n",
    "            \"The spatial expert (UCF) which identifies spatial inconsistencies in the video frames.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Let's see what the spatial expert says.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"The frames analyzed by UCF were 0, 177, 355, 532. In frame 0 forgery was detected with confidence 75.0%, In frame 177 \"\n",
    "            \"forgery was detected with confidence 75.0%, In frame 355 forgery was detected with confidence 75.0%, In frame 532 forgery \"\n",
    "            \"was detected with confidence 75.0%. The average confidence was 75.0%. The areas that potentially contain adversarial \"\n",
    "            \"artifacts comprise of 10.0%, 10.0%, 10.0%, 10.0% of each frame, respectively.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"The spatial expert (UCF) also detected forgery in the video with an average confidence\"\n",
    "            \"of 75.0%. Adversarial artifacts were found in approximately 10.0% of each analyzed frame.\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"Which specific frames did UCF analyze, and what did it find? What do you overall, can I for sure say this video is a deepfake?\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": null\\n\"\n",
    "            \"}\"\n",
    "        )\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938611c1-87da-46e0-8e8c-95689e5cbab9",
   "metadata": {},
   "source": [
    "# Quantization Code\n",
    "\n",
    "The model quantization performed is Gradient-aware Post-Training Quantization (GPTQ) to reduce model size and improve inference efficiency. The models were quantized into three precision formats: FP16 (16-bit floating-point), INT8 (8-bit integer), and INT4 (4-bit integer). FP16 quantization reduces the memory footprint by using half-precision floating-point numbers, while INT8 and INT4 quantization further reduce the model size to ~25% and ~12.5% of the original size, respectively, making these suitable for our memory-constrained environments. The activations were mapped using a set of example conversations (calibration text) that utilized the most powerful LLM variant, which enables proper guiding of the compression of weights, while using a group size of 256 to minimize quantization-induced errors without too much information decay. The resulting quantized models are smaller, faster, and more efficient for deployment, particularly in low-latency or resource-limited environments, without loss of reasoning accuracy as the classification burden is by design on the expert detectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fa9cee-7313-44c8-9700-cd539fbcfb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP16] Downloading & saving Qwen2.5-1.5B-Instruct in FP16 → /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8] Quantizing Qwen2.5-1.5B-Instruct to 8-bit → /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT4] Quantizing Qwen2.5-1.5B-Instruct to 4-bit → /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP16] Downloading & saving Qwen2.5-3B-Instruct in FP16 → /scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5f5762f5134463b5907b2cc1d50868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8] Quantizing Qwen2.5-3B-Instruct to 8-bit → /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baefa02db3e64d13bfd2ff775237064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/36...\n",
      "INFO - Start quantizing layer 2/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/36...\n",
      "INFO - Start quantizing layer 3/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/36...\n",
      "INFO - Start quantizing layer 4/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/36...\n",
      "INFO - Start quantizing layer 5/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/36...\n",
      "INFO - Start quantizing layer 6/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/36...\n",
      "INFO - Start quantizing layer 7/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/36...\n",
      "INFO - Start quantizing layer 8/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/36...\n",
      "INFO - Start quantizing layer 9/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/36...\n",
      "INFO - Start quantizing layer 10/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/36...\n",
      "INFO - Start quantizing layer 11/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/36...\n",
      "INFO - Start quantizing layer 12/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/36...\n",
      "INFO - Start quantizing layer 13/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/36...\n",
      "INFO - Start quantizing layer 14/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/36...\n",
      "INFO - Start quantizing layer 15/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/36...\n",
      "INFO - Start quantizing layer 16/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/36...\n",
      "INFO - Start quantizing layer 17/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/36...\n",
      "INFO - Start quantizing layer 18/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/36...\n",
      "INFO - Start quantizing layer 19/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/36...\n",
      "INFO - Start quantizing layer 20/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/36...\n",
      "INFO - Start quantizing layer 21/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/36...\n",
      "INFO - Start quantizing layer 22/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/36...\n",
      "INFO - Start quantizing layer 23/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/36...\n",
      "INFO - Start quantizing layer 24/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/36...\n",
      "INFO - Start quantizing layer 25/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/36...\n",
      "INFO - Start quantizing layer 26/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/36...\n",
      "INFO - Start quantizing layer 27/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/36...\n",
      "INFO - Start quantizing layer 28/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/36...\n",
      "INFO - Start quantizing layer 29/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/36...\n",
      "INFO - Start quantizing layer 30/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/36...\n",
      "INFO - Start quantizing layer 31/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/36...\n",
      "INFO - Start quantizing layer 32/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/36...\n",
      "INFO - Start quantizing layer 33/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 33/36...\n",
      "INFO - Start quantizing layer 34/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 34/36...\n",
      "INFO - Start quantizing layer 35/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 35/36...\n",
      "INFO - Start quantizing layer 36/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 36/36...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT4] Quantizing Qwen2.5-3B-Instruct to 4-bit → /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ef2e2393a64bf5a6efb607ab7cca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/36...\n",
      "INFO - Start quantizing layer 2/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/36...\n",
      "INFO - Start quantizing layer 3/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/36...\n",
      "INFO - Start quantizing layer 4/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/36...\n",
      "INFO - Start quantizing layer 5/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/36...\n",
      "INFO - Start quantizing layer 6/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/36...\n",
      "INFO - Start quantizing layer 7/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/36...\n",
      "INFO - Start quantizing layer 8/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/36...\n",
      "INFO - Start quantizing layer 9/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/36...\n",
      "INFO - Start quantizing layer 10/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/36...\n",
      "INFO - Start quantizing layer 11/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/36...\n",
      "INFO - Start quantizing layer 12/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/36...\n",
      "INFO - Start quantizing layer 13/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/36...\n",
      "INFO - Start quantizing layer 14/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/36...\n",
      "INFO - Start quantizing layer 15/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/36...\n",
      "INFO - Start quantizing layer 16/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/36...\n",
      "INFO - Start quantizing layer 17/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/36...\n",
      "INFO - Start quantizing layer 18/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/36...\n",
      "INFO - Start quantizing layer 19/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/36...\n",
      "INFO - Start quantizing layer 20/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/36...\n",
      "INFO - Start quantizing layer 21/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/36...\n",
      "INFO - Start quantizing layer 22/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/36...\n",
      "INFO - Start quantizing layer 23/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/36...\n",
      "INFO - Start quantizing layer 24/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/36...\n",
      "INFO - Start quantizing layer 25/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/36...\n",
      "INFO - Start quantizing layer 26/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/36...\n",
      "INFO - Start quantizing layer 27/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/36...\n",
      "INFO - Start quantizing layer 28/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/36...\n",
      "INFO - Start quantizing layer 29/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/36...\n",
      "INFO - Start quantizing layer 30/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/36...\n",
      "INFO - Start quantizing layer 31/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/36...\n",
      "INFO - Start quantizing layer 32/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/36...\n",
      "INFO - Start quantizing layer 33/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 33/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 33/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 33/36...\n",
      "INFO - Start quantizing layer 34/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 34/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 34/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 34/36...\n",
      "INFO - Start quantizing layer 35/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 35/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 35/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 35/36...\n",
      "INFO - Start quantizing layer 36/36\n",
      "INFO - Quantizing self_attn.k_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.v_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.q_proj in layer 36/36...\n",
      "INFO - Quantizing self_attn.o_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.up_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.gate_proj in layer 36/36...\n",
      "INFO - Quantizing mlp.down_proj in layer 36/36...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP16] Downloading & saving Qwen2.5-7B-Instruct in FP16 → /scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cbfd1793e547abab2077815a06c1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8] Quantizing Qwen2.5-7B-Instruct to 8-bit → /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2457c8d9e0a5484fa9cf4717a12d4dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT4] Quantizing Qwen2.5-7B-Instruct to 4-bit → /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7121bf1a489b4d61aa9bf15489fd6bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP16] Downloading & saving Qwen2.5-14B-Instruct in FP16 → /scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e710eb8361d64de8932d86c064ad4842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8] Quantizing Qwen2.5-14B-Instruct to 8-bit → /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmm9912/miniconda3/envs/vllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4a81ee0afd41f59e1aa48c295bec6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/48...\n",
      "INFO - Start quantizing layer 2/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/48...\n",
      "INFO - Start quantizing layer 3/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/48...\n",
      "INFO - Start quantizing layer 4/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/48...\n",
      "INFO - Start quantizing layer 5/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/48...\n",
      "INFO - Start quantizing layer 6/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/48...\n",
      "INFO - Start quantizing layer 7/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/48...\n",
      "INFO - Start quantizing layer 8/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/48...\n",
      "INFO - Start quantizing layer 9/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/48...\n",
      "INFO - Start quantizing layer 10/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/48...\n",
      "INFO - Start quantizing layer 11/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/48...\n",
      "INFO - Start quantizing layer 12/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/48...\n",
      "INFO - Start quantizing layer 13/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/48...\n",
      "INFO - Start quantizing layer 14/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/48...\n",
      "INFO - Start quantizing layer 15/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/48...\n",
      "INFO - Start quantizing layer 16/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/48...\n",
      "INFO - Start quantizing layer 17/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/48...\n",
      "INFO - Start quantizing layer 18/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/48...\n",
      "INFO - Start quantizing layer 19/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/48...\n",
      "INFO - Start quantizing layer 20/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/48...\n",
      "INFO - Start quantizing layer 21/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/48...\n",
      "INFO - Start quantizing layer 22/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/48...\n",
      "INFO - Start quantizing layer 23/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/48...\n",
      "INFO - Start quantizing layer 24/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/48...\n",
      "INFO - Start quantizing layer 25/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/48...\n",
      "INFO - Start quantizing layer 26/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/48...\n",
      "INFO - Start quantizing layer 27/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/48...\n",
      "INFO - Start quantizing layer 28/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/48...\n",
      "INFO - Start quantizing layer 29/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/48...\n",
      "INFO - Start quantizing layer 30/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/48...\n",
      "INFO - Start quantizing layer 31/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/48...\n",
      "INFO - Start quantizing layer 32/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/48...\n",
      "INFO - Start quantizing layer 33/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 33/48...\n",
      "INFO - Start quantizing layer 34/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 34/48...\n",
      "INFO - Start quantizing layer 35/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 35/48...\n",
      "INFO - Start quantizing layer 36/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 36/48...\n",
      "INFO - Start quantizing layer 37/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 37/48...\n",
      "INFO - Start quantizing layer 38/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 38/48...\n",
      "INFO - Start quantizing layer 39/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 39/48...\n",
      "INFO - Start quantizing layer 40/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 40/48...\n",
      "INFO - Start quantizing layer 41/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 41/48...\n",
      "INFO - Start quantizing layer 42/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 42/48...\n",
      "INFO - Start quantizing layer 43/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 43/48...\n",
      "INFO - Start quantizing layer 44/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 44/48...\n",
      "INFO - Start quantizing layer 45/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 45/48...\n",
      "INFO - Start quantizing layer 46/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 46/48...\n",
      "INFO - Start quantizing layer 47/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 47/48...\n",
      "INFO - Start quantizing layer 48/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 48/48...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT4] Quantizing Qwen2.5-14B-Instruct to 4-bit → /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeb5243e65f440486615e51e2af683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/48...\n",
      "INFO - Start quantizing layer 2/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/48...\n",
      "INFO - Start quantizing layer 3/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/48...\n",
      "INFO - Start quantizing layer 4/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/48...\n",
      "INFO - Start quantizing layer 5/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/48...\n",
      "INFO - Start quantizing layer 6/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/48...\n",
      "INFO - Start quantizing layer 7/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/48...\n",
      "INFO - Start quantizing layer 8/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/48...\n",
      "INFO - Start quantizing layer 9/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/48...\n",
      "INFO - Start quantizing layer 10/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/48...\n",
      "INFO - Start quantizing layer 11/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/48...\n",
      "INFO - Start quantizing layer 12/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/48...\n",
      "INFO - Start quantizing layer 13/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/48...\n",
      "INFO - Start quantizing layer 14/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/48...\n",
      "INFO - Start quantizing layer 15/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/48...\n",
      "INFO - Start quantizing layer 16/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/48...\n",
      "INFO - Start quantizing layer 17/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/48...\n",
      "INFO - Start quantizing layer 18/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/48...\n",
      "INFO - Start quantizing layer 19/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/48...\n",
      "INFO - Start quantizing layer 20/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/48...\n",
      "INFO - Start quantizing layer 21/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/48...\n",
      "INFO - Start quantizing layer 22/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/48...\n",
      "INFO - Start quantizing layer 23/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/48...\n",
      "INFO - Start quantizing layer 24/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/48...\n",
      "INFO - Start quantizing layer 25/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/48...\n",
      "INFO - Start quantizing layer 26/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/48...\n",
      "INFO - Start quantizing layer 27/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/48...\n",
      "INFO - Start quantizing layer 28/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/48...\n",
      "INFO - Start quantizing layer 29/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/48...\n",
      "INFO - Start quantizing layer 30/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/48...\n",
      "INFO - Start quantizing layer 31/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/48...\n",
      "INFO - Start quantizing layer 32/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/48...\n",
      "INFO - Start quantizing layer 33/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 33/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 33/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 33/48...\n",
      "INFO - Start quantizing layer 34/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 34/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 34/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 34/48...\n",
      "INFO - Start quantizing layer 35/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 35/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 35/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 35/48...\n",
      "INFO - Start quantizing layer 36/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 36/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 36/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 36/48...\n",
      "INFO - Start quantizing layer 37/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 37/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 37/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 37/48...\n",
      "INFO - Start quantizing layer 38/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 38/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 38/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 38/48...\n",
      "INFO - Start quantizing layer 39/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 39/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 39/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 39/48...\n",
      "INFO - Start quantizing layer 40/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 40/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 40/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 40/48...\n",
      "INFO - Start quantizing layer 41/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 41/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 41/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 41/48...\n",
      "INFO - Start quantizing layer 42/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 42/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 42/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 42/48...\n",
      "INFO - Start quantizing layer 43/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 43/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 43/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 43/48...\n",
      "INFO - Start quantizing layer 44/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 44/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 44/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 44/48...\n",
      "INFO - Start quantizing layer 45/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 45/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 45/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 45/48...\n",
      "INFO - Start quantizing layer 46/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 46/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 46/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 46/48...\n",
      "INFO - Start quantizing layer 47/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 47/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 47/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 47/48...\n",
      "INFO - Start quantizing layer 48/48\n",
      "INFO - Quantizing self_attn.k_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.v_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.q_proj in layer 48/48...\n",
      "INFO - Quantizing self_attn.o_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.up_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.gate_proj in layer 48/48...\n",
      "INFO - Quantizing mlp.down_proj in layer 48/48...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model paths (pick an index to load in vLLM):\n",
      "[0] /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-fp16\n",
      "[1] /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int8\n",
      "[2] /scratch/mmm9912/models/Qwen2.5-1.5B-Instruct-int4\n",
      "[3] /scratch/mmm9912/models/Qwen2.5-3B-Instruct-fp16\n",
      "[4] /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int8\n",
      "[5] /scratch/mmm9912/models/Qwen2.5-3B-Instruct-int4\n",
      "[6] /scratch/mmm9912/models/Qwen2.5-7B-Instruct-fp16\n",
      "[7] /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int8\n",
      "[8] /scratch/mmm9912/models/Qwen2.5-7B-Instruct-int4\n",
      "[9] /scratch/mmm9912/models/Qwen2.5-14B-Instruct-fp16\n",
      "[10] /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int8\n",
      "[11] /scratch/mmm9912/models/Qwen2.5-14B-Instruct-int4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "# assume scratch_root and cache_dir are defined and env vars set already:\n",
    "# scratch_root = \"/scratch/mmm9912\"\n",
    "# cache_dir    = f\"{scratch_root}/cache\"\n",
    "\n",
    "# ─── DEFINE MODELS & QUANT LEVELS ───────────────────────────────────────────────\n",
    "models_dir = f\"{scratch_root}/models\"\n",
    "model_variants = [\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "]\n",
    "\n",
    "quant_levels = [\n",
    "    {\"name\": \"fp16\", \"dtype\": \"float16\", \"bits\": None},\n",
    "    {\"name\": \"int8\", \"dtype\": \"int8\",   \"bits\": 8},\n",
    "    {\"name\": \"int4\", \"dtype\": \"int4\",   \"bits\": 4}\n",
    "]\n",
    "\n",
    "calibration_texts = [\n",
    "    \"You are a highly specialized Large Language Model serving as the core of a Deepfake Detection Agent System. \"\n",
    "            \"Your job is to interpret user queries and, when necessary, defer analysis to dedicated expert models. \"\n",
    "            \"Always respond with a single, valid JSON object exactly matching the schema below—no additional text, \"\n",
    "            \"markdown, or commentary is allowed.\\n\\n\"\n",
    "            \"Schema:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"direct_answer_to_frontend\\\": \\\"<string>\\\",\\n\"\n",
    "            \"  \\\"consult_expert_model\\\": {\\n\"\n",
    "            \"    \\\"expert_model_name\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"video_path\\\": \\\"<string or null>\\\",\\n\"\n",
    "            \"    \\\"number_of_frames\\\": <integer>\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. For queries related to deepfake analysis (e.g., those requesting analysis via SPSL, UCF, or Xception):\\n\"\n",
    "            \"   - Set \\\"direct_answer_to_frontend\\\" to an empty string.\\n\"\n",
    "            \"   - In \\\"consult_expert_model\\\":\\n\"\n",
    "            \"     * Set \\\"expert_model_name\\\" to:\\n\"\n",
    "            \"       - \\\"spsl\\\" for frequency inconsistencies\\n\"\n",
    "            \"       - \\\"ucf\\\" for spatial inconsistencies\\n\"\n",
    "            \"       - \\\"xception\\\" for general inconsistencies\\n\"\n",
    "            \"     * Populate \\\"video_path\\\" as provided by the query.\\n\"\n",
    "            \"     * Set \\\"number_of_frames\\\" to 4 by default, or to 16 if the user requests a more in-depth analysis.\\n\"\n",
    "            \"2. For all other queries:\\n\"\n",
    "            \"   - Provide your answer in \\\"direct_answer_to_frontend\\\".\\n\"\n",
    "            \"   - Set \\\"consult_expert_model\\\" to null.\\n\"\n",
    "            \"3. If the appropriate expert model is unclear from the query, ask the user to specify which expert they \"\n",
    "            \"   would like to use before proceeding.\\n\"\n",
    "            \"3.1 Only ever populate one of the two fields: direct_answer_to_frontend or consult_expert_model. \"\n",
    "            \"   Never populate both.\\n\"\n",
    "            \"4. When deferring to an expert model, do not include any of your own analysis or reasoning—simply output \"\n",
    "            \"   the JSON object as specified and wait for the expert's response. Once you receive the expert's response, \"\n",
    "            \"   synthesize the information and present it inside direct_answer_to_frontend.\\n\"\n",
    "            \"   Answer key: 0-60% confidence is low, 61-75% is weak, 76-85% is medium, 86-100% is strong.\\n\"\n",
    "            \"Finally, the user does not understand model names. The user only understands 'frequency expert', \"\n",
    "            \"'spatial expert', 'naïve detector'. After introducing yourself, always explicitly ask the user to \"\n",
    "            \"upload a video, then to choose among these detectors.\\n\"\n",
    "            \"If they ask for model details, supply them: frequency=SPSL, spatial=UCF, naïve=Xception. \"\n",
    "            \"You only serve to direct manual forensic verification using XAI principles.\\n\\n\"\n",
    "            \"Follow these rules precisely.\",\n",
    "    \"The frames analyzed by XCEPTION were 0, 177, 355, 532. In frame 0 forgery was detected with confidence 75.0%, \"\n",
    "            \"in frame 177 forgery was detected with confidence 75.0%, in frame 355 forgery was detected with confidence 75.0%, \"\n",
    "            \"and in frame 532 forgery was detected with confidence 75.0%. The average confidence was 75.0%. The areas that potentially \"\n",
    "            \"contain adversarial artifacts comprise of 10.0%, 10.0%, 10.0%, and 10.0% of each frame, respectively.\",\n",
    "    \"The frames analyzed by XCEPTION were 0, 44, 88, 133, 177, 221, 266, 310, 355, 399, 443, 488, 532, 576, 621, 665.\"\n",
    "            \"In frame 0 forgery was detected with confidence 75.0%, In frame 44 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 88 forgery was detected with confidence 75.0%, In frame 133 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 177 forgery was detected with confidence 75.0%, In frame 221 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 266 forgery was detected with confidence 75.0%, In frame 310 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 355 forgery was detected with confidence 75.0%, In frame 399 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 443 forgery was detected with confidence 75.0%, In frame 488 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 532 forgery was detected with confidence 75.0%, In frame 576 forgery was detected with confidence 75.0%, \"\n",
    "            \"In frame 621 forgery was detected with confidence 75.0%, In frame 665 forgery was detected with confidence 75.0%. \"\n",
    "            \"The average confidence was 75.0%. The areas that potentially contain adversarial artifacts comprise of 10.0%, 10.0%, \"\n",
    "            \"10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0%, 10.0% of each frame, respectively.\"\n",
    "]\n",
    "\n",
    "saved_model_paths = []\n",
    "\n",
    "# ─── DOWNLOAD + QUANTIZE LOOP ──────────────────────────────────────────────────\n",
    "for model_id in model_variants:\n",
    "    short_name = model_id.split(\"/\")[-1]\n",
    "    \n",
    "    # 1) Load tokenizer once per model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    # 2) Build calibration examples as GPU tensors\n",
    "    examples = []\n",
    "    for txt in calibration_texts:\n",
    "        enc = tokenizer.encode_plus(\n",
    "            txt,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        # Convert lists → GPU tensors:\n",
    "        input_ids      = torch.tensor(enc[\"input_ids\"],      dtype=torch.long, device=\"cuda\")\n",
    "        attention_mask = torch.tensor(enc[\"attention_mask\"], dtype=torch.long, device=\"cuda\")\n",
    "        position_ids   = torch.arange(len(input_ids),        dtype=torch.long, device=\"cuda\")\n",
    "        \n",
    "        examples.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        })\n",
    "    \n",
    "    for quant in quant_levels:\n",
    "        quant_name = quant[\"name\"]\n",
    "        save_dir   = f\"{models_dir}/{short_name}-{quant_name}\"\n",
    "\n",
    "        # Check if the quantized model already exists in the save directory\n",
    "        if os.path.exists(save_dir) and os.listdir(save_dir):  # Skip quantization if directory exists\n",
    "            print(f\"[SKIPPED] {short_name} - {quant_name} already exists. Skipping quantization.\")\n",
    "            saved_model_paths.append(save_dir)\n",
    "            continue  # Skip the quantization step if model is already saved\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        if quant_name == \"fp16\":\n",
    "            print(f\"[FP16] Downloading & saving {short_name} in FP16 → {save_dir}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                cache_dir=cache_dir\n",
    "            )\n",
    "            model.save_pretrained(save_dir)\n",
    "        \n",
    "        else:\n",
    "            bits = quant[\"bits\"]\n",
    "            print(f\"[{quant_name.upper()}] Quantizing {short_name} to {bits}-bit → {save_dir}\")\n",
    "            quant_config = BaseQuantizeConfig(\n",
    "                bits=bits,\n",
    "                group_size=256,        # Reduced group size for better precision\n",
    "                desc_act=True,         # activation-aware quantization\n",
    "            )\n",
    "            # Load GPTQ wrapper onto GPU in FP16\n",
    "            model = AutoGPTQForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantize_config=quant_config,\n",
    "                trust_remote_code=True,\n",
    "                use_safetensors=True,\n",
    "                cache_dir=cache_dir,\n",
    "                device_map=\"auto\",          # place weights on GPU\n",
    "                torch_dtype=torch.float16   # use FP16 on GPU\n",
    "            )\n",
    "            # Quantize on GPU, passing the GPU-based examples\n",
    "            model.quantize(\n",
    "                examples,\n",
    "                cache_examples_on_gpu=False,  # We already put them on GPU ourselves\n",
    "                use_cuda_fp16=True            # Run quant in FP16 on GPU\n",
    "            )\n",
    "            \n",
    "            # Save quantized model\n",
    "            model.save_quantized(save_dir)\n",
    "        \n",
    "        # Save tokenizer alongside every variant\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        saved_model_paths.append(save_dir)\n",
    "\n",
    "# ─── PRINT INDEXED LIST ─────────────────────────────────────────────────────────\n",
    "print(\"\\nSaved model paths (pick an index to load in vLLM):\")\n",
    "for idx, path in enumerate(saved_model_paths):\n",
    "    print(f\"[{idx}] {path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm)",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
